<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>odps上自定义UDAF-java开发</title>
    <url>/2021/03/24/odps%E4%B8%8A%E8%87%AA%E5%AE%9A%E4%B9%89UDAF%E5%BC%80%E5%8F%91/</url>
    <content><![CDATA[<p>一:写在前面<br><br>&#160;&#160;&#160;&#160;最近在研究odps的udf开发，不得不说maxcompute对hive的封装真的够狠的，用户需要关心的东西很少。UDF就不多说了，odps和hive没有什么差别，都是继承udf然后重写evaluate()就ok。本文主要介绍一下udaf的开发。<br><br>&#160;&#160;&#160;&#160;首先简单说一下udaf是什么吧。udaf是udf的一个子类。UDF(User Defined Function用户自定义函数)顾名思义就是用户自己用java或者python实现的，封装成一个函数可以到数据库中运行的函数。UDF中还包括UDF，UDAF(User Defined Aggregation),UDTF(User-Defined Table-Generating Functions）。<br><br>&#160;&#160;&#160;&#160;1.UDF指单行输入，单行输出的函数，我们常用的concat(),to_char()都是这样的函数。<br><br>&#160;&#160;&#160;&#160;2.UDAF指多行输入，单行输出的函数，一般和group by连用，常用的有sum(),avg()等等。<br><br>&#160;&#160;&#160;&#160;3.UDAF指单行输入，多行输出的函数，比较少用，用的比较多的有explode()<br><br>&#160;&#160;&#160;&#160;在通常的udaf-java开发流程中，需要继承AbstractGenericUDAFResolver，实现步骤十分复杂，而在odps中，我们只需要继承Aggregator，重点实现 merge(), iterate(), terminate() 这三个方法即可。<br><br>&#160;&#160;&#160;&#160;这里我们用一个Mysql里面十分常见，但是Hive中没有的函数group_concat()函数的UDAF实现来说明开发的流程。<br><br>        （注:本文已经默认你安装了MaxCompute-IDEA所需要的东西并连接了自己的odps,具体过程可见<a href="https://help.aliyun.com/document_detail/50892.html">https://help.aliyun.com/document_detail/50892.html</a>)<br></p>
<p> &#160;&#160;&#160;&#160;二:目的<br><br>&#160;&#160;&#160;&#160;以表A为例子</p>
<table>
<thead>
<tr>
<th>id</th>
<th>device</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>ios</td>
</tr>
<tr>
<td>1</td>
<td>android</td>
</tr>
<tr>
<td>2</td>
<td>ios</td>
</tr>
<tr>
<td>1</td>
<td>ios</td>
</tr>
</tbody></table>
<p>&#160;&#160;&#160;&#160;实现如下聚合:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> id,group_concat(device) <span class="keyword">from</span> tableA <span class="keyword">group</span> <span class="keyword">by</span> id</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>id</th>
<th>device</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>ios,android,ios</td>
</tr>
<tr>
<td>2</td>
<td>ios</td>
</tr>
</tbody></table>
<p>&#160;&#160;&#160;&#160;三.具体实现:<br><br>&#160;&#160;&#160;&#160;根据mapreduce的工作原理，我们可以画一张流程图。把iterate想象成map()函数，merge想象成combine(),terminate想象成reduce()函数。</p>
<p><img src="/images/pasted-1.png" alt="upload successful"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">代码如下</span><br><span class="line">package com.yupaopao.udaf;</span><br><span class="line"></span><br><span class="line">import com.aliyun.odps.io.Text;</span><br><span class="line">import com.aliyun.odps.io.Writable;</span><br><span class="line">import com.aliyun.odps.udf.UDFException;</span><br><span class="line">import com.aliyun.odps.udf.Aggregator;</span><br><span class="line">import com.aliyun.odps.udf.annotation.Resolve;</span><br><span class="line">import java.io.DataInput;</span><br><span class="line">import java.io.DataOutput;</span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * 定义输入输出类型</span><br><span class="line"> *&#x2F;</span><br><span class="line">@Resolve(&#123;&quot;String-&gt;String&quot;&#125;)</span><br><span class="line">public class GroupConcat extends Aggregator &#123;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 继承Writable接口 自定义buffer进行序列化</span><br><span class="line">     *&#x2F;</span><br><span class="line">    private static class GroupConcatBuffer implements Writable &#123;</span><br><span class="line">        private String str &#x3D; &quot;&quot;;</span><br><span class="line">        @Override</span><br><span class="line">        public void write(DataOutput out) throws IOException &#123;</span><br><span class="line">            out.writeChars(str);</span><br><span class="line">        &#125;</span><br><span class="line">        &#x2F;**</span><br><span class="line">         * Writable的readFields方法， 由于partial的writable对象是重用的，</span><br><span class="line">         * 同一个对象的readFields方法会被调用多次。该方法每次调用的时候重置整个对象，如果对象中包含Collection，则需要清空。</span><br><span class="line">         *&#x2F;</span><br><span class="line">        @Override</span><br><span class="line">        public void readFields(DataInput in) throws IOException &#123;</span><br><span class="line">            str &#x3D; in.readLine();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 声明最终结果变量</span><br><span class="line">     *&#x2F;</span><br><span class="line">    private Text ret &#x3D; new Text();</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 创建初始返回结果的值</span><br><span class="line">     *&#x2F;</span><br><span class="line">    @Override</span><br><span class="line">    public Writable newBuffer() &#123;</span><br><span class="line">        return new GroupConcat.GroupConcatBuffer();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * @param buffer 为一个阶段的汇总数据(在不同map()中汇总出来的group by数据)</span><br><span class="line">     * @param args 表示一行数据 其中args[0]表示传入的第一列(一般仅传入一列)</span><br><span class="line">     * 每次新增一条数据 就把传入的列放入目前已有的累积字符串中。</span><br><span class="line">     *&#x2F;</span><br><span class="line">    @Override</span><br><span class="line">    public void iterate(Writable buffer, Writable[] args) throws UDFException &#123;</span><br><span class="line">        Text arg &#x3D; (Text) args[0];</span><br><span class="line">        GroupConcat.GroupConcatBuffer buf &#x3D; (GroupConcat.GroupConcatBuffer) buffer;</span><br><span class="line">        if (arg !&#x3D; null) &#123;</span><br><span class="line">            buf.str&#x3D;buf.str+arg+&quot;,&quot;;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;**</span><br><span class="line">     * @param buffer  聚合buffer</span><br><span class="line">     * @param partial 其他分片聚合结果</span><br><span class="line">     * 在这个方法中，对不同map()的结果进行汇总</span><br><span class="line">     *&#x2F;</span><br><span class="line">    @Override</span><br><span class="line">    public void merge(Writable buffer, Writable partial) throws UDFException &#123;</span><br><span class="line">        GroupConcat.GroupConcatBuffer buf &#x3D; (GroupConcat.GroupConcatBuffer) buffer;</span><br><span class="line">        GroupConcat.GroupConcatBuffer p &#x3D; (GroupConcat.GroupConcatBuffer) partial;</span><br><span class="line">        buf.str &#x3D; buf.str+ p.str;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 设置最终结果，同时把留在最后面的逗号去掉。</span><br><span class="line">     * @param buffer</span><br><span class="line">     * @return Object UDAF的最终结果</span><br><span class="line">     *&#x2F;</span><br><span class="line">    @Override</span><br><span class="line">    public Writable terminate(Writable buffer) throws UDFException &#123;</span><br><span class="line">        GroupConcatBuffer buf &#x3D; (GroupConcatBuffer) buffer;</span><br><span class="line">        if (buf.str&#x3D;&#x3D;&quot;&quot;)&#123;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            String result &#x3D; buf.str.substring(0, buf.str.length()-1);</span><br><span class="line">            ret.set(result);</span><br><span class="line">        &#125;</span><br><span class="line">        return ret;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>&#160;&#160;&#160;&#160;四:<br><br>&#160;&#160;&#160;&#160;打包右键点击写好的java文件，选择deploy to server打包<br></p>
<p><img src="/images/pasted-3.png" alt="upload successful"></p>
<p>&#160;&#160;&#160;&#160;点击右上方的MaxCompute，选择添加资源，把资源上传到odps<br></p>
<p><img src="/images/pasted-4.png" alt="upload successful"></p>
<p>&#160;&#160;&#160;&#160;点击右上方的MaxCompute，选择创建UDF，起个名字，把udf注册进去(注意:这里使用的名字就是最后在odps里使用的名)<br><br><img src="/images/pasted-5.png" alt="upload successful"></p>
<p>&#160;&#160;&#160;&#160;五:<br><br>&#160;&#160;&#160;&#160;函数使用。成功！<br><br> <img src="/images/pasted-2.png" alt="upload successful"></p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>缓慢变化维的问题背景及解决办法</title>
    <url>/2021/03/30/%E7%BC%93%E6%85%A2%E5%8F%98%E5%8C%96%E7%BB%B4%E7%9A%84%E9%97%AE%E9%A2%98%E8%83%8C%E6%99%AF%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
    <content><![CDATA[<p>&#160;&#160;&#160;&#160;在KimBall所提出并被各IT企业所推崇的维度建模方法中，数仓中的表被划分成了维度表和事实表。对维度建模的具体内容，这里暂不多赘述(后续会更新),今天先讨论一下为什么会出现缓慢变化维问题和怎么去解决这个问题。<br><br>&#160;&#160;&#160;&#160;维度表作为事实表的维度补充，一般有扁平型(宽且数据量较少),数据稳定(比如说城市维度表，几乎不可能产生改变)的特点。但是这两种特征并不能囊括所有维度表，虽然维度表的大多数属性都相对稳定，但它们很多时候也不是一成不变的（比分说城市改名，更改行政区域，改变相当缓慢但总会随时间改变)。当然，对业务方来说可能并不关心这些变化，但是未雨绸缪，数仓需要制定相应的策略来应对这些变化。而这些会发生缓慢变化的维度，就被称为缓慢变化维SCD（Slowly Changing Dimension)。以下介绍几种常见缓慢变化维类型及对应的解决办法。<br><br>&#160;&#160;&#160;&#160;维度表作为事实表的维度补充，一般有扁平型(宽且数据量较少),数据稳定(比如说城市维度表，几乎不可能产生改变)的特点。但是这两种特征并不能囊括所有维度表，虽然维度表的大多数属性都相对稳定，但它们很多时候也不是一成不变的（比分说城市改名，更改行政区域，改变相当缓慢但总会随时间改变)。当然，对业务方来说可能并不关心这些变化，但是未雨绸缪，数仓需要制定相应的策略来应对这些变化。而这些会发生缓慢变化的维度，就被称为缓慢变化维SCD（Slowly Changing Dimension)。以下介绍几种常见缓慢变化维类型及对应的解决办法。<br><br></p>
<p>&#160;&#160;&#160;&#160;<font size=5><strong>类型0:保留原值</strong></font><br><br>&#160;&#160;&#160;&#160;维度属性绝对不出现变化，出现变化可能是脏数据或者认定为脏数据。因此事实始终按照原始值分组。不需要做额外处理。<br><br>&#160;&#160;&#160;&#160;例如：用户信息维度的性别属性。<br><br><br>&#160;&#160;&#160;&#160;<font size=5><strong>类型1:重写</strong></font><br><br>&#160;&#160;&#160;&#160;维度属性发生变化，但业务方不关心该属性变化，接受用新属性覆盖原属性的方式来对事实分组。这种时候只需要以当前值重写先前存在的值，不需要触碰事实表。这种方法实现最容易，但是弊端是无法跟踪历史变化，同时采用一段时间后，部分历史变化记录将丢失。<br><br>&#160;&#160;&#160;&#160;例如：广告信息维度，广告名由于投放人员操作失误有变动，但是业务人员只关心最新广告名。<br><br><br>&#160;&#160;&#160;&#160;<font size=5><strong>类型2:增加新行</strong></font><br><br>&#160;&#160;&#160;&#160;维度属性发生变化，并且业务方关心该属性变化。可以增加两个新列create_time创建时间和expire_time失效时间，每当维度属性更新，就创建新创建一行记录改更新，并将之前行的expire_time记录为更新时间。这样维度属性的变化便有迹可循，业务方只需要加上时间关联即可追朔。这种方法优点是历史信息不会丢失，但是实现起来比较麻烦，对业务人员的使用要求提高。<br><br>&#160;&#160;&#160;&#160;例如:用户信息维度：用户的所在城市发生变化，业务人员关心用户以前和现在的居住地，可用增加新行的方式追踪用户所在地。<br><br><br>&#160;&#160;&#160;&#160;<font size=5><strong>类型3:增加新属性</strong></font><br><br>&#160;&#160;&#160;&#160;维度属性发生变化，并且业务方希望同时对比当前维度属性和先前维度属性。可以新增属性：_old属性来记录这一变化，并且保留原来的属性。这种方法优点是简单易用的同记录了历史变化，缺点是不可以解决无法预测的属性变化，同时当属性变化速度太快时，新属性过多导致维度膨胀。<br><br>&#160;&#160;&#160;&#160;例如:部门业绩维度。企业可能发生部门重组，保留当前属性和以前属性可以同时观察部门重组前后的事实变化。<br><br><br>&#160;&#160;&#160;&#160;<font size=5><strong>类型4:增加微型维度</strong></font><br><br>&#160;&#160;&#160;&#160;维度属性发生变化，该维度表比较大，同时变化率比较快，同时业务方需要大致追踪历史变化的时候。如果采用方法2，3，大表的频繁新增行和列增加会带来性能负担，因此可以考虑将部分维度信息建设成微型维度表。<br>&#160;&#160;&#160;&#160;例如:用户信息维度，用户的收入变化较频繁，可以将部分信息从用户信息维度中退化到事实表里（一个人口普查微型维度的外键），然后建一个人口普查微型维度表来记录不同的收入区间。<br><br><br><br>&#160;&#160;&#160;&#160;<font size=5><strong>类型5:微型维度与类型1支架表</strong></font><br><br>&#160;&#160;&#160;&#160;希望在缺乏事实度量时获得当前概要计数，或者希望基于客户当前概要上卷历史事实的时候可以考虑采用。在维度表中即时更新属性，同时另外建设一张微型维度表作为历史变化数据的补充。<br><br>&#160;&#160;&#160;&#160;例如：用户信息维度，建一个人口普查微型维度表来记录不同的收入区间，同时保留”收入”这一维度并即时更新。<br><br><br><br>&#160;&#160;&#160;&#160;<font size=5><strong>类型6:将类型1属性增加到类型2维度</strong></font><br><br>&#160;&#160;&#160;&#160;这种方式实质上用了类型1+类型2+类型3。维度属性更新，在维度表中增加一个属性用来记录当前属性值，并保留过去属性值(同时存在)，然后用create_time和expire_time记录生效和过期时间。<br><br>&#160;&#160;&#160;&#160;例如：感觉新旧属性有关联的要求，同时有时效要求的时候可以使用，但是感觉对用户来说比较复杂，目前没有想到非用不可的场景。<br><br><br><br>&#160;&#160;&#160;&#160;<font size=5><strong>类型7:双重类型1与类型2维度</strong></font><br><br>&#160;&#160;&#160;&#160;在类型6的基础上把事实发生时候的维度属性和当前维度属性分别做成维度表表，基于历史维度表和当前维度表连接和过滤事实。<br><br>&#160;&#160;&#160;&#160;例如：笔者在使用的过程中确实遇到过这种场景。其实就是广告发生信息维度表+广告当前信息维度表。感觉还是比类型6合理一些。<br><br><br>&#160;&#160;&#160;&#160;<font size=5><strong>总结:</strong></font><br><br>&#160;&#160;&#160;&#160;缓慢变化维度在维度建模中其实是蛮常见的情况，但是解决方式从来不是唯一的，也很少出现只用一种类型的情况，根据业务变化和要求找到最优解才是王道。最后贴一张「数据仓库工具箱」的缓慢变化维度总结，帮助快速了解。<br><br></p>
<table>
<thead>
<tr>
<th>id</th>
<th>SCD类型</th>
<th>维度表行动</th>
<th>对事实分析的影响</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>类型0</td>
<td>属性无变化</td>
<td>事实与属性的原始值关联</td>
</tr>
<tr>
<td>1</td>
<td>类型1</td>
<td>重写属性</td>
<td>事实与当前属性关联</td>
</tr>
<tr>
<td>2</td>
<td>类型2</td>
<td>为新属性值增加新维度行</td>
<td>事实与事实发生时的有效属性关联</td>
</tr>
<tr>
<td>3</td>
<td>类型3</td>
<td>增加新列来保存属性当前和原先的值</td>
<td>事实与当前和先前属性交替关联</td>
</tr>
<tr>
<td>4</td>
<td>类型4</td>
<td>增加包含快速变化属性的微型维度</td>
<td>事实与事实发生时的有效属性关联</td>
</tr>
<tr>
<td>5</td>
<td>类型5</td>
<td>增加类型4微型维度以及在基本维度中重写类型1微型维度键</td>
<td>事实与事实发生时的有效属性关联再加当前快速变化的属性值</td>
</tr>
<tr>
<td>6</td>
<td>类型6</td>
<td>在类型2维度行中增加类型1重写属性，并重写先前的属性行</td>
<td>事实与事实发生时的有效属性关联，加上当前值</td>
</tr>
<tr>
<td>7</td>
<td>类型7</td>
<td>增加包含新属性值的类型2维度行，加上限于当前行和属性值的视图</td>
<td>事实与事实发生时的有效属性关联，加上当前值</td>
</tr>
</tbody></table>
<p><br><br></p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>DataWarehouse</tag>
      </tags>
  </entry>
  <entry>
    <title>最终幻想7重置版--写在通关后的第329天(涉及剧透)</title>
    <url>/2021/03/26/%E6%9C%80%E7%BB%88%E5%B9%BB%E6%83%B37%E9%87%8D%E7%BD%AE%E7%89%88-%E5%86%99%E5%9C%A8%E9%80%9A%E5%85%B3%E5%90%8E%E7%9A%84%E7%AC%AC329%E5%A4%A9/</url>
    <content><![CDATA[<p>&#160;&#160;&#160;&#160;2020年在我的游戏史中注定是浓墨重彩的一年。初期疫情影响，后期进入职场，让我有大把时间体验电子游戏，这一年玩到的佳作也特别多:女神异闻录5，异度神剑2，如龙7，勇者斗恶龙11S等等等等，但是丝毫不影响最终幻想7重置版的出彩。</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-12.png width = "75%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">诞生之前就已是传说</font> 
</center>
&#160;&#160;&#160;&#160;老实说，我完全跟最终幻想7老玩家这个群体沾不上边，没有办法想象我认识的人中会有系列玩家。和最终幻想7最早的交集是2016年上大学买了游戏本在游民星空的游戏库里搜寻盗版单机游戏的时候，发现明明最终幻想已经出到十几了，这个七代还是在好评榜前列，但是当时搜了一下发现这游戏甚至是上个世纪开发的东西，就完全提不起兴趣。<br>
&#160;&#160;&#160;&#160;接下来，已经到了2020年，我稍微玩了点单机大作，也稍微会关注游戏资讯的时候。这两年间，重制版大作层出不穷，我记得很清楚，当时在涩谷过马路的时候，一抬头就可以看见生化危机2重制版的巨幅海报。同年还有魔兽争霸3重制版，生化危机3重制版等等重制版大作。所以对我来说「最终幻想7重制版-第一章」只不过是另一款跟其他情怀大作没有什么区别的重制游戏罢了，再加上这个「第一章」的副标题，在我这里刻上了像生化危机3般又短情怀又不够的标签。
<br>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-8.png width = "75%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">生化危机2重制版海报</font> 
</center>
&#160;&#160;&#160;&#160;然后是游戏发售当天，以a9vg为首的游戏媒体转载了游戏的宣传片，宣传片中蒂法，爱丽丝的建模确实非常惊艳，战斗系统的演示也不差，才让我有一点点对这个游戏改观。最后击中我的是一张情怀浓浓的图片，图片里九头身高清克劳德牵着像素小人克劳德，旁白"Let's save the world again"。那晚上我转发了这条微博后立马打开psn购入了这款游戏，然后就是50+个小时，20天的unknown journey。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-13.png width = "45%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">Let's save the world again</font> 
</center>
&#160;&#160;&#160;&#160;跟大行其道的主流3A比，「最终幻想7重制版-第一章」并不算一款快节奏的游戏，即便有接近欧美厂商的画面技术和全球宣发造势，它在内核上还是有相当深刻JRPG的影子：节奏缓慢，人物美型，追求数值而策略而不是操作，甚至很多支线小任务的设计也存在着和JRPG一致的弊端。<br>
&#160;&#160;&#160;&#160;在剧情上，我虽然是新玩家，但是购入游戏等待下载的时候，我已经在知乎/贴吧/游戏媒体等地大致了解了原作剧情:包括神罗公司的背景，爱丽丝之死，克劳德的情况，甚至连CC里面扎克斯的故事和后传圣子降临有了解。坦白说最终幻想的主线剧情，讨论星球和生命的议题，牺牲和平等的议题，在1997年可能非常新颖，但是经过20多年影视作品也好，其他游戏也好，已经把同样的议题翻来覆去讨论了太多遍。如果只说第一章的话，其实就是一个简单的环保组织反抗过度开采星球能源神罗公司的故事，其中米德加的游戏背景，上层和下层的生活情况和贫富差别很像「铳梦」，最近也在电影「阿丽塔：战斗天使」里呈现了。因此，本身「最终幻想7重制版-第一章」的剧情，应该是不太值得期待的。但是野村哲也这个鬼才，可以说是重新定义了重制。既然老玩家都知道剧情，原来的剧情可能由于时代原因难以吸引新玩家，那就把重制版做成戏说最终幻想7。游戏主线剧情不变，加入了元素「菲拉」暗示角色命运的改变，每次角色即将做出和原作剧情不符合的行为的时候，「菲拉」就会跳出来阻止并触发战斗，把角色的行为纠正回原作。其实在游戏中，从结果看「菲拉」的加入对原作的剧情没有改动，但是当菲拉被打败后，却引入了新的可能性：角色的命运即将改写，1997年的最终幻想7已经无法阻止他们了。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-14.png width = "75%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">命运修正者:菲拉</font> 
</center>
&#160;&#160;&#160;&#160;为什么野村哲也会想到用命运改变的形式来重制剧情？我认为最大的原因是，最终幻想7中爱丽丝的死亡和最终幻想7核心危机中扎克斯的死亡，可能是系列玩家心中最大的遗憾，在这上面做文章，最容易俘获老玩家的心。<br>
&#160;&#160;&#160;&#160;先讲讲爱丽丝之死，在1997年，3D游戏的先驱Square Enix第一次将大型3D游戏投入市场的时候，游戏大体还是青少年的消费品，剧情停留在魔法与剑，骑士与公主的故事，主要用文字刻画。在这些少年少女眼里，爱丽丝在游戏前半段是伙伴，是团队里唯一的奶妈，是注定和主角团一起迎接胜利的角色。同时，游戏前半段也花费了大幅的篇幅去刻画爱丽丝的性格，从教堂相遇到Forgotten City，一步步让玩家喜欢上这个像素3D小人。在游戏中间，用萨菲罗斯一把从天而降的大剑在玩家面前刺死爱丽丝，同时Aerith's Theme响起，玩家与萨菲罗斯战斗后播放克劳德水葬爱丽丝cg，而后一张碟结束。玩家起身到电视机前去换碟，给了悲伤驻足的时间。有趣的是，爱丽丝之死是真正的死亡，没有复活，JRPG的玩家应该都知道奶妈的重要性，爱丽丝一离队对游戏后半程的体验肯定是有影响的，制作者选择让爱丽丝担任这个角色，也是在游戏后半段一遇到血量困难的情况，会想起来“如果爱丽丝在就好了”。从结果上看，爱丽丝之死的设计无疑是成功的，全球无数玩家都在谣传复活爱丽丝的方法，我本人也在网路上看到诸如把碟换回来换回去就能复活爱丽丝的谣言。那么，当角色的命运改变，是不是就给了爱丽丝之死新的可能性？当年在全球玩家口中的谣言，会不会变成真的？我想只有野村哲也和北范佳濑知道了。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-15.png width = "75%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">爱丽丝之死</font> 
</center>]]></content>
      <categories>
        <category>Game</category>
      </categories>
      <tags>
        <tag>final fantasy</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop-MapReduce流程</title>
    <url>/2021/03/26/Hadoop-MapReduce%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<p>&#160;&#160;&#160;&#160;本文主要放一下之前学习mapreduce的笔记。也给自己创造一个再次阅读，巩固的机会。首先声明：本人看的Hadoop源码不多，内容主要摘自各博客，然后自己总结精简得出<br><br>￼<br><img src="/images/pasted-6.png" alt="upload successful"><br>&#160;&#160;&#160;&#160;MapReduce包括Mapper（Mapper类)阶段和Reducer(Reducer类)阶段，其实说白了就是两个实现类，其中Map阶段和Reduce阶段都包含部分Shuffle阶段工作，因此也有人把mapreduce流程说成map-&gt;shuffle-&gt;reduce。</p>
<p><font size=5><strong>Map阶段</strong></font><br>&#160;&#160;&#160;&#160;1.输入分片 input split: 一个大的文件会根据block块切分成多个分片(这里生成多少个分片可以是用户自定义的，只需要通过hive参数设定每个block的大小即可)。每个输入分片会让一个map进程来处理。<br>&#160;&#160;&#160;&#160;2.Map任务:<br>&#160;&#160;&#160;&#160; i. 初始化: 创建context，map.class实例，设置输入输出，创建mapper的上下文Map 任务把分片传递给 TaskTracker 里面的 InputFormat类的 createRecordReader() 方法以便获取分片的 RecordReader 对象,由RecordReader读取分片数据并转成键值对形式。<br>&#160;&#160;&#160;&#160;ii. 执行:  执行 Mapper class.的run()方法，这个方法可以被重写，对每个分片里的数据执行一样的方法。(比如说 给每个key生成一个value=1)<br>3.shuffle(Map端):<br>&#160;&#160;&#160;&#160;i.溢写:map会使用Mapper.Context.write()将map函数的输出溢写到内存中的环形缓冲区，同时调用Partitioner类对数据分区，每一个分区对应一个reducer。当缓冲区即将充满(80%)，溢写会被执行(并行，缓冲区可以继续写入，当溢写未完成而缓冲区已满，map会停止写入内存)具体过程<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;a.创建一个溢写记录SpillRecord 和一个FSOutputStream 文件输出流（本地文件系统）<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;b.内存内排序缓冲中的块：输出的数据会使用快排算法按照partitionIdx, key排序<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;c.分区序列化写到磁盘,序列化(快排)之前可进行combiner(可选)，方便排序<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;d.merge合并溢写文件,如果合并后文件较大，可以再进行一次combiner<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;e.再进行一次Partitioner对数据分区 (为什么要两次呢,因为这个时候可能进行了两次combiner,应该分配到每个reducer上的数据出现变化)<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;ii. combiner合并阶段(可选,有两次触发时机):简单地合并key值<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;a.减少传到reducer的数据量（即通过减少key-value对减少网络传输)<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;b.减轻reducer的负担<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;c.缺点 job没有依赖，有磁盘io开销<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;iii.Partitioner(补充): <br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;a.Hadoop Mapreduce默认的Partitioner是Hash Partitioner,对key计算hash值并分区<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;b.TotalOrderPartitioner 按照数据大小分区 常用于全局排序 按照大小将数据分成多个区间，后一个区间的数据一定大于前一个区间<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;c. Partitioner可以被重写，克服低效分区，解决数据倾斜和全局排序问题<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;iv. 排序原因:<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;a.方便reducer找到自己分区的数据<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; b.减轻reducer中排序负担<br><br><font size=5><strong>Reduce阶段</strong></font><br>&#160;&#160;&#160;&#160;1.shuffle(Reduce端):<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;i.copy: Reduce端并行的从多个map下载该reduce对应的partition部分数据（通过HTTP)<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;ii.merge(内存到磁盘): Reduce将map结果下载到本地时，需要把多个mapper输入流合并成一个,一直在进行，直到从map下载数据完毕。<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;iii.split: Reduce对下载下来的map数据，会先缓存在内存中,当内存达到一定用量时写入硬盘(这里也可以进行combiner)<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;iv.merge(磁盘到磁盘)：map数据下载过程中，一个后台线程把这些文件合并为更大的，有序的文件，也就是进行全局排序，为最后的最终合并减少工作量(多轮)<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;v. 最终merge(方式不确定): map数据下载完毕后，所有数据被合并成一个整体有序的文件作为Reduce的输入。有可能是都来自于硬盘，有可能都来自于内存，也有可能两边都有<br>&#160;&#160;&#160;&#160;2.Reduce任务:<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;i.自己实现逻辑 Map阶段的map方法类似 通常做聚合操作<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;ii.OutputCollector.collect() 方法把 reduce 任务的输出结果写到 HDFS</p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>2021-03-29 周报</title>
    <url>/2021/03/29/2021-03-29-%E5%91%A8%E6%8A%A5/</url>
    <content><![CDATA[<p>&#160;&#160;&#160;&#160;这周租的房间城管拆掉，无奈回家了一趟，目前居住酒店，所以可以写的内容也会比较少。<br><br><font size=5><strong>Big Data</strong></font><br><br>&#160;&#160;&#160;&#160;1.阅读「数据仓库工具箱」「仓库」章节，深入了解缓慢变化维的问题背景和解决方式，企业数据仓库总线架构。<br><br>&#160;&#160;&#160;&#160;2.了解MPP的OLAP系统Doris，脱胎于百度的Palo,在2018年捐献给Apache基金会。doris使用场景与Clickhouse相似(处理高质量的结构化数据)，但是特性和原理都有所不同。<br></p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-16.png width = "75%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">主流OLAP引擎对比1</font> 
</center>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-17.png width = "75%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">主流OLAP引擎对比2</font> 
</center>

<p>&#160;&#160;&#160;&#160;可以看到Clickhouse和Doris都能支撑OLAP需要的大部分场景，但是Clickhouse的并发量一直是个问题。同时Doris支持Mysql访问协议，跟clickhouse的非标协议接口相比，上手起来会更加舒服。但是也需要注意，Doris在2018年开始开源，目前社区非常不活跃，遇到问题可能难以寻求帮助。搭建上我感觉差不多麻烦吧，卧龙凤雏。<br><br>&#160;&#160;&#160;&#160;顺便也讲讲原理差别，除了OLAP常见的列存储，向量化执行外，Doris还有多数据模型(预聚合，K-V)的特点，为不同查询场景定制。而Clickhouse的编码压缩，多索引比较牛逼。<br><br><font size=5><strong>Game</strong></font><br><br>&#160;&#160;&#160;&#160;1.本周购买了「MONSTER HUNTER RISE」，之前其实玩过「MONSTER HUNTER WORLD」比较失望，只玩了差不多一小时就没动过了，所以这次买的ns卡带，还没有发货。<br><br>&#160;&#160;&#160;&#160;2.非常想玩双人成行，Josef Fares的游戏「双子传说」「逃出生天」都非常有意思，目前也算是我觉得个人风格最吸引人的游戏制作人，嘴挺臭但是确实有两把手的。可是这游戏需要两人玩，然后拥有PS4/PS5的人还比较少，慢慢等吧。<br></p>
<p><font size=5><strong>Others</strong></font><br><br>&#160;&#160;&#160;&#160;1.这周终于把博客建起来了，用的是Hexo+Next，使用体验还行,准备把以前沉淀的一些文章也搬过来，后续要坚持一下。<br><br>&#160;&#160;&#160;&#160;2.做题进度:<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;#剑指 Offer 31 栈的压入、弹出序列<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;#剑指 Offer 32 - I 从上到下打印二叉树</p>
]]></content>
      <categories>
        <category>周报</category>
      </categories>
      <tags>
        <tag>Logs</tag>
      </tags>
  </entry>
  <entry>
    <title>ClickHouse中的几种JOIN方式</title>
    <url>/2021/04/01/ClickHouse%E4%B8%AD%E7%9A%84%E5%87%A0%E7%A7%8DJOIN%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<p>&#160;&#160;&#160;&#160;ClickHouse中的SQL很多时候跟MySQL的访问协议是不兼容的，个人在使用的时候感觉大概有20%的SQL语法有区别。今天主要讲讲ClickHouse支持的所有JOIN方式，其中有跟MySQL一样的，也有一些ClickHouse独家支持的。<br><br>&#160;&#160;&#160;&#160;<font size=5><strong>Join方式汇总</strong></font><br><br>&#160;&#160;与MySQL一致: INNER JOIN, LEFT OUTER JOIN, RIGHT OUTER JOIN, FULL OUTER JOIN,CROSS JOIN<br><br>&#160;&#160;&#160;&#160;与HIVE一致: LEFT SEMI JOIN, RIGHT SEMI JOIN<br><br>&#160;&#160;&#160;&#160;ClickHouse独家支持: LEFT ANY JOIN / RIGHT ANY JOIN / ANY JOIN, LEFT ASOF JOIN / ASOF JOIN, GLOBAL JOIN<br><br>&#160;&#160;&#160;&#160;<font size=5><strong>Join方式详解</strong></font><br><br>&#160;&#160;&#160;&#160;1.<strong>INNER JOIN</strong>: 只返回左右表中key都匹配的行。同MySQL一样。<br><br>&#160;&#160;&#160;&#160;2.<strong>LEFT OUTER JOIN</strong>: 除了匹配的行之外，还返回左表中的非匹配行，同MySQL一样。<br><br>&#160;&#160;&#160;&#160;3.<strong>RIGHT OUTER JOIN</strong>: 除了匹配的行之外，还返回右表中的非匹配行，同MySQL一样。<br><br>&#160;&#160;&#160;&#160;4.<strong>FULL OUTER JOIN</strong>: 除了匹配的行之外，还会返回两个表中的非匹配行。同MySQL一样。<br><br>&#160;&#160;&#160;&#160;5.<strong>CROSS JOIN</strong>: 产生整个表的笛卡尔积，不需要指定join key。同MySQL一样。<br><br>&#160;&#160;&#160;&#160;6.<strong>LEFT SEMI JOIN</strong>: 返回左表中所有能和右表的join key匹配的行，不产生笛卡尔积(相当于 in 右表子查询 )。同Hive一样。<br></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tab1 <span class="keyword">LEFT</span> SEMI <span class="keyword">JOIN</span> tab2 <span class="keyword">on</span> tab1.id<span class="operator">=</span>tab2.id</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>等价于</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tab1 <span class="keyword">where</span> tab1.id <span class="keyword">in</span> (<span class="keyword">select</span> id <span class="keyword">from</span> tab2)</span><br></pre></td></tr></table></figure>
<p>&#160;&#160;&#160;&#160;7.<strong>RIGHT SEMI JOIN</strong>: 返回右表中所有能和左表的join key匹配的行，不产生笛卡尔积(相当于 in 左表子查询 )。同Hive一样。<br></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tab1 <span class="keyword">RIGHT</span> SEMI <span class="keyword">JOIN</span> tab2 <span class="keyword">on</span> tab1.id<span class="operator">=</span>tab2.id</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>等价于</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tab2 <span class="keyword">where</span> tab2.id <span class="keyword">in</span> (<span class="keyword">select</span> id <span class="keyword">from</span> tab1)</span><br></pre></td></tr></table></figure>
<p>&#160;&#160;&#160;&#160;8.<strong>LEFT SEMI JOIN</strong>: 返回右表中所有能和左表的join key匹配的行，不产生笛卡尔积(相当于 in 左表子查询 )。同Hive一样。<br></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tab1 <span class="keyword">RIGHT</span> SEMI <span class="keyword">JOIN</span> tab2 <span class="keyword">on</span> tab1.id<span class="operator">=</span>tab2.id</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>等价于</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tab2 <span class="keyword">where</span> tab2.id <span class="keyword">in</span> (<span class="keyword">select</span> id <span class="keyword">from</span> tab1)</span><br></pre></td></tr></table></figure>
<p>&#160;&#160;&#160;&#160;9.<strong>LEFT ANY JOIN / RIGHT ANY JOIN / ANY JOIN</strong>: 连接两表，出现笛卡尔积的时候最多只返回2条。Clickhouse特性。<br></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tab1  <span class="keyword">ANY</span> <span class="keyword">JOIN</span> tab2 <span class="keyword">on</span> tab1.id<span class="operator">=</span>tab2.id</span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>等价于HIVE</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> </span><br><span class="line">(</span><br><span class="line">	<span class="keyword">select</span> <span class="operator">*</span>，</span><br><span class="line">    <span class="built_in">row_number</span>()<span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> tab1.id) <span class="keyword">as</span> rn </span><br><span class="line">    <span class="keyword">from</span> tab1 </span><br><span class="line">    <span class="keyword">JOIN</span> tab2 </span><br><span class="line">    <span class="keyword">on</span> tab1.id<span class="operator">=</span>tab2.id</span><br><span class="line">)<span class="keyword">where</span> rn<span class="operator">&lt;=</span><span class="number">2</span>;</span><br></pre></td></tr></table></figure>
<p>&#160;&#160;&#160;&#160;10.<strong>LEFT ASOF JOIN / ASOF JOIN</strong>: 连接两表，接受不等值连接。Clickhouse特性。<br></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tab1  <span class="keyword">ANY</span> <span class="keyword">JOIN</span> tab2 <span class="keyword">on</span> tab1.id<span class="operator">&lt;=</span>tab2.id</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>等价于HIVE</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tab1  <span class="keyword">ANY</span> <span class="keyword">JOIN</span> tab2 <span class="keyword">on</span> tab1.id<span class="operator">&lt;=</span>tab2.id</span><br></pre></td></tr></table></figure>
<p>&#160;&#160;&#160;&#160;11.<strong>GLOBAL JOIN</strong>: 分布式连接。Clickhouse特性。<br><br>&#160;&#160;&#160;&#160;讲GLOBAL JOIN的时候必须讲讲GLOBAL JOIN和普通JOIN的执行区别。<br><br>&#160;&#160;&#160;&#160;当使用正常 JOIN，将查询发送到远程服务器。 为了创建正确的表，在每个子查询上运行子查询，并使用此表执行联接。换句话说，在每个服务器上单独形成右表。子查询将开始在每个远程服务器上运行。由于子查询使用分布式表，所以每个远程服务器上的子查询将会对每个远程服务器都感到不满，如果您有一个100个服务器集群，执行整个查询将需要10000个基本请求，这通常被认为是不可接受的举个例子。下面用SQL的形式讲解。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>一个普通的<span class="keyword">join</span>，分布式查询发起。</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tab1 <span class="keyword">JOIN</span> tab2 <span class="keyword">on</span> tab1.id<span class="operator">=</span>tab2.id</span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>发送到每个远程服务器上，每个远程服务器生成右表，分布式查询替换成远程服务器查询。</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tab1_local <span class="keyword">JOIN</span> tab2_local <span class="keyword">on</span> tab1_local.id<span class="operator">=</span>tab2_local.id</span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>因为每个远程服务器中的可能没有所有数据，所以每个远程服务器中生成的右表都不完整，这个时候该服务器需要向除自己外的所有远程服务器请求右表中的其余数据，才能完整地连接本服务器上的所有数据(这里觉得不好理解的话，可以想一下map<span class="operator">-</span>reduce中的common <span class="keyword">join</span>，为什么必须启动reduce来完成),因此这个时候就产生了远程服务器数<span class="operator">^</span><span class="number">2</span>的请求数。</span><br></pre></td></tr></table></figure>
<p>&#160;&#160;&#160;&#160;而使用GLOBAL JOIN时，首先请求者服务器运行查询并创建一个临时表（右表）。 此临时表将传递到每个远程服务器，并使用传输的临时数据对其运行连接。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>一个<span class="keyword">global</span> <span class="keyword">join</span>，分布式查询发起。</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tab1 <span class="keyword">global</span> <span class="keyword">JOIN</span> tab2 <span class="keyword">on</span> tab1.id<span class="operator">=</span>tab2.id</span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>请求服务器先生成一个完整的右表临时表。</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tab </span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>该右表这里命名为tmp_tab,然后发送到所有的远程服务器上。然后每个远程服务器运行。</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tab1  <span class="keyword">JOIN</span> tmp_tab <span class="keyword">on</span> tab1.id<span class="operator">=</span>tmp_tab.id</span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>这个时候每个远程服务器中的左表都能完整地连接右表，然后再进行后续的合并。(这里觉得不好理解的话，可以想一下map<span class="operator">-</span>reduce中的hash <span class="keyword">join</span>是怎么不启动reduce完成<span class="keyword">join</span>的。)</span><br></pre></td></tr></table></figure>
<p>&#160;&#160;&#160;&#160;建议在Clickhouse中不要使用普通的join，直接替换成GLOBAL JOIN即可。</p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>ClickHouse</tag>
      </tags>
  </entry>
  <entry>
    <title>2021-04-06周报</title>
    <url>/2021/04/06/2021-04-06%E5%91%A8%E6%8A%A5/</url>
    <content><![CDATA[<p>&#160;&#160;&#160;&#160;这周清明假期，所以周报晚了一天。这周需求有点多，所以阅读时间有点变少，更多的是在coding，刷题和需求评审上。然后假期也花了点时间调整一下被房东搞崩了的心态。跟金姐焊接吃了个饭聊了下，确实想在回去读书前休息一阵，做点自己想做的事情，计划做个游戏商品相关的大数据项目，还有想去澳门的酒店玩一下，真的特别便宜。<br><br><font size=5><strong>Big Data</strong></font><br><br>&#160;&#160;&#160;&#160;1.阅读「数据仓库工具箱」「订单管理」章节。<br><br>&#160;&#160;&#160;&#160;2.读了Clickhouse官方文档中的几种JOIN方式的差别。<br></p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-18.png width = "75%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">Clickhouse的几种JOIN方式</font> 
</center>
这一块的重点集中在ANY JOIN,ASOF JOIN和GLOBAL JOIN上，ANY JOIN实现了对笛卡尔积的消除，ASOF JOIN实现不等值连接，GLOBAL JOIN体现了clickhouse的分布式思想，实际使用中建议用global join 代替join。<br>
&#160;&#160;&#160;&#160;3.ODPS-UDF JAVA开发，subArray函数实现。
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">subArrayUDF</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line">    <span class="comment">// TODO define parameters and return type, e.g:  public String evaluate(String a, String b)</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;String&gt; <span class="title">evaluate</span><span class="params">(List&lt;String&gt; arraylist, String left, String right)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> length = arraylist.size();</span><br><span class="line">        <span class="keyword">if</span> (length==<span class="number">0</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">int</span> i = Integer.parseInt(left);</span><br><span class="line">        <span class="keyword">int</span> j = Integer.parseInt(right);</span><br><span class="line">        <span class="keyword">if</span> (j&gt;length)&#123;</span><br><span class="line">            j=length;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> arraylist.subList(i,j);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><font size=5><strong>Game</strong></font><br><br>&#160;&#160;&#160;&#160;1.「MONSTER HUNTER RISE」令人失望，以后怪物猎人系列的作品应该不会再买。<br><br>&#160;&#160;&#160;&#160;2.「双人成行」，今年为止的最佳游戏，有预感在会在tga上拿到最佳提名(但是能不能获奖还得看今年其他作品的表现)，最佳家庭游戏应该没什么问题。跟一个微博认识的女网友一起玩的，体验很好，小惊喜不断。预计下个周末通关，到时候会写一篇详细测评。<br><br>&#160;&#160;&#160;&#160;3.「年轮」(剧本)，蛮不错的推理本，成功还原的时候一个年轮的样子在我心里浮现，有点击中我。不过拿的本有点边缘，情感体验一般。<br></p>
<p><font size=5><strong>Others</strong></font><br><br>&#160;&#160;&#160;&#160;1.阅读「<font color=blue><a href="https://www.mihunye.com/science/51575.html">低代码开发，老树开新芽</a></font>」。还蛮同意低代码是未来的，预计5年内会被大范围使用。不过仅仅局限在初创/小成本项目，并且主要集中在前端。项目一大遇到的问题复杂程度是低代码在短时间内难以解决的。<br><br>&#160;&#160;&#160;&#160;2.做题进度:<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;#103 二叉树的锯齿形层序遍历<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;#104 二叉树的最大深度<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;#剑指 Offer 32 - II. 从上到下打印二叉树 II<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;#剑指 Offer 32 - III. 从上到下打印二叉树 III<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;#剑指 Offer 33. 二叉搜索树的后序遍历序列<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;#剑指 Offer 33. 二叉搜索树的后序遍历序列<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;#剑指 Offer 35. 复杂链表的复制<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;#206. 反转链表<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;剑指 Offer 39. 数组中出现次数超过一半的数字</p>
]]></content>
      <categories>
        <category>周报</category>
      </categories>
      <tags>
        <tag>Logs</tag>
      </tags>
  </entry>
  <entry>
    <title>OneData与企业数据仓库总线架构的异同</title>
    <url>/2021/04/09/OneData%E4%B8%8E%E4%BC%81%E4%B8%9A%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%80%BB%E7%BA%BF%E6%9E%B6%E6%9E%84%E7%9A%84%E5%BC%82%E5%90%8C/</url>
    <content><![CDATA[<p>&#160;&#160;&#160;&#160;OneData是阿里巴巴内部进行数据整合及管理的方法体系和工具。阿里巴巴的大数据工程师在这一体系下，构建统一、规范、可共享的全域数据体系，避免数据的冗余和重复建设，规避数据烟囱和不一致性，充分发挥阿里巴巴在大数据海量、多样性方面的独特优势。其中，OneData还分为了OneModel,OneID和OneService。OneData的核心思想在于“统一”：统一口径规范，统一数据资产管理，统一接口服务化。<br><br>&#160;&#160;&#160;&#160;本文所提的「企业数据仓库总线架构」，指的是「数据仓库工具箱」的「库存」篇中的企业数据仓库总线架构。阅读过程中发现其核心思想与OneData是一致的。企业数据仓库总线架构的关键目标是消除烟筒式开发，避免数据孤岛，建立一致性维度同时强调统一的数据治理。强调的也是“统一”的思想。<br><br>&#160;&#160;&#160;&#160;那么OneData和企业数据仓库总线架构除了在构建思想上相似外还有哪些异同呢?</p>
<p>&#160;&#160;&#160;&#160;<font size=5><strong>相同点</strong></font><br><br>&#160;&#160;&#160;&#160;1.思想相同，在与企业数据的统一。<br><br>&#160;&#160;&#160;&#160;2.目标相同，都在规避数据烟筒和不一致性，减少重复开发和冗余。<br><br>&#160;&#160;&#160;&#160;2.行动相似，都要求对数据定义，标示，领域值达成一致，确保大家用同一种语言交谈。<br></p>
<p>&#160;&#160;&#160;&#160;<font size=5><strong>不同点</strong></font><br><br>&#160;&#160;&#160;&#160;1.出发点不同。企业数据仓库总线架构的出发点是数据仓库，只关心数仓建设方面的事情。OneData出发点是数据中台，指导的是除了数据仓库外，上层的算法模型，服务输出等数据中台职能。<br><br>&#160;&#160;&#160;&#160;1.范围不同。OneData包含了企业数据仓库总线架构，同时把这种方法引伸到了其他方面。<br></p>
<p>&#160;&#160;&#160;&#160;<font size=5><strong>OneData实施流程</strong></font><br><br>&#160;&#160;&#160;&#160;1.建模。规范化模型分层、数据流向和主题划分，从而降低研发成本，增强指标复用性，并提高业务的支撑能力。<br><br>&#160;&#160;&#160;&#160;2.主题划分。聚焦业务，对业务进行拆分后面向分析输出。<br><br>&#160;&#160;&#160;&#160;3.规范。表命名规范，字段命名规范，指标定义规范，维度表事实表划分和建设规范。<br><br>&#160;&#160;&#160;&#160;4.统一输出。统一模型输出，接口输出，用统一的方式对数据进行资产管理。<br></p>
<p>&#160;&#160;&#160;&#160;<font size=5><strong>企业数据仓库总线架构实施流程</strong></font><br><br>&#160;&#160;&#160;&#160;1.绘制企业数据仓库总线矩阵。通过统一的矩阵来识别业务过程，用以支持一致性维度的建设和提炼一致性维度和关键业务过程的关系。<br><br>&#160;&#160;&#160;&#160;2.一致性维度建设(公共维度)。<br><br>&#160;&#160;&#160;&#160;3.强调数据治理和管理，针对数据定义，标示，领域值达成一致。<br><br>&#160;&#160;&#160;&#160;4.建立一致性事实定义。<br></p>
]]></content>
  </entry>
  <entry>
    <title>2021-04-12周报</title>
    <url>/2021/04/12/2021-04-12%E5%91%A8%E6%8A%A5/</url>
    <content><![CDATA[<p>&#160;&#160;&#160;&#160;有亿点点无聊的一周，需求比较紧密，去一家小公司面试被几道sql题难住了，Hql还是有蛮大进步空间的，然后网络的知识有点记不清，最近找时间复习一下。吃了23度不太冷椰子鸡，还可以吧，但是还是最想吃一绪和八库。周末和网友玩双人成行，已经差不多快通关了，泡了一下午书店，还玩了「最终幻想XV」，比预期中好多了。对了买了西湖音乐节的票，好久没去音乐节，期待！<br><br><font size=5><strong>Big Data</strong></font><br><br>&#160;&#160;&#160;&#160;1.阅读「<font color=blue><a href="https://my.oschina.net/dataclub/blog/4689656">如何使用阿里的OneData方法论</a>顺便复习了KIMBALL的企业数据仓库总线模型。</font>」<br><br>&#160;&#160;&#160;&#160;2.按照教程在IDEA上搭建Flink，并实现了一个简单的wordcount。<br><br>&#160;&#160;&#160;&#160;3.使用python爬取了steam上的部分游戏信息。<br></p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-19.png width = "75%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">steam游戏信息</font> 
</center>
目前能获取的信息还比较有限，发售日期和系统也需要迭代一下。现分享下现行代码，后续会更新优化版。

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">num = <span class="number">0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_text</span>(<span class="params">url</span>):</span> </span><br><span class="line">    <span class="keyword">try</span>: </span><br><span class="line">        headers = &#123; <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) &#x27;</span> <span class="string">&#x27;Chrome/85.0.4183.102 Safari/537.36&#x27;</span>, <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;zh-CN &#x27;</span> &#125; </span><br><span class="line">        r = requests.get(url, headers=headers)</span><br><span class="line">        r.raise_for_status() </span><br><span class="line">        r.encoding = r.apparent_encoding </span><br><span class="line">        <span class="keyword">return</span> r.text</span><br><span class="line">    <span class="keyword">except</span>: </span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;爬取网站失败！&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">game_info, jump_link, game_evaluation, text,Game_release_time,Game_os</span>):</span> </span><br><span class="line">    soup = BeautifulSoup(text, <span class="string">&quot;html.parser&quot;</span>) <span class="comment"># </span></span><br><span class="line">    <span class="comment">#游戏评价 </span></span><br><span class="line">    w = soup.find_all(class_=<span class="string">&quot;col search_reviewscore responsive_secondrow&quot;</span>) </span><br><span class="line">    <span class="keyword">for</span> u <span class="keyword">in</span> w: </span><br><span class="line">        <span class="keyword">if</span> u.span <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: </span><br><span class="line">            game_evaluation.append( u.span[<span class="string">&quot;data-tooltip-html&quot;</span>].split(<span class="string">&quot;&lt;br&gt;&quot;</span>)[<span class="number">0</span>] + <span class="string">&quot;,&quot;</span> + u.span[<span class="string">&quot;data-tooltip-html&quot;</span>].split(<span class="string">&quot;&lt;br&gt;&quot;</span>)[-<span class="number">1</span>]) </span><br><span class="line">            </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            game_evaluation.append(<span class="string">&quot;暂无评价！&quot;</span>) </span><br><span class="line">            </span><br><span class="line">    <span class="comment">#支持os</span></span><br><span class="line">    o = soup.find_all(class_=<span class="string">&quot;col search_name ellipsis&quot;</span>) </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> o:</span><br><span class="line">        tmp=[]</span><br><span class="line">        <span class="keyword">if</span> (<span class="number">1</span>==<span class="number">1</span>):</span><br><span class="line">            tmp=[]</span><br><span class="line">            tmp = re.findall(<span class="string">&quot;platform_img...&quot;</span>,<span class="built_in">str</span>(item.p))</span><br><span class="line">            <span class="built_in">print</span>(tmp)</span><br><span class="line"><span class="comment">#         except:</span></span><br><span class="line"><span class="comment">#             tmp.append(&#x27;unknown&#x27;)</span></span><br><span class="line">            </span><br><span class="line">        Game_os.append(tmp)</span><br><span class="line">  </span><br><span class="line">            </span><br><span class="line">    <span class="comment"># 游戏详情页面链接 </span></span><br><span class="line">    link_text = soup.find_all(<span class="string">&quot;div&quot;</span>, <span class="built_in">id</span>=<span class="string">&quot;search_resultsRows&quot;</span>) </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> link_text:</span><br><span class="line">        b = k.find_all(<span class="string">&#x27;a&#x27;</span>) </span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> b: jump_link.append(j[<span class="string">&#x27;href&#x27;</span>]) </span><br><span class="line"></span><br><span class="line">    <span class="comment">#发售时间</span></span><br><span class="line">    release_text = soup.find_all(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&quot;col search_released responsive_secondrow&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> time <span class="keyword">in</span> release_text:</span><br><span class="line">        <span class="keyword">try</span>: </span><br><span class="line">            Game_release_time.append(time.string.strip());</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            Game_release_time.append(<span class="string">&#x27;unknown&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 名字和价格 </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">global</span> num</span><br><span class="line">    name_text = soup.find_all(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&quot;responsive_search_name_combined&quot;</span>) </span><br><span class="line">    <span class="keyword">for</span> z <span class="keyword">in</span> name_text: </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每个游戏的价格 </span></span><br><span class="line">        name = z.find(class_=<span class="string">&quot;title&quot;</span>).string.strip() </span><br><span class="line">        <span class="comment"># 判断折扣是否为None，提取价格 </span></span><br><span class="line">        <span class="keyword">if</span> z.find(class_=<span class="string">&quot;col search_discount responsive_secondrow&quot;</span>).string <span class="keyword">is</span> <span class="literal">None</span>: </span><br><span class="line">            price = z.find(class_=<span class="string">&quot;col search_price discounted responsive_secondrow&quot;</span>).text.strip().split(<span class="string">&quot;₩&quot;</span>) </span><br><span class="line"></span><br><span class="line">            game_info.append([num+<span class="number">1</span> , name, price[<span class="number">2</span>].strip(), game_evaluation[num], jump_link[num],Game_release_time[num],Game_os[num]]) </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            price = z.find(class_=<span class="string">&quot;col search_price responsive_secondrow&quot;</span>).string.strip().split(<span class="string">&quot;₩&quot;</span>) </span><br><span class="line">            game_info.append([num+<span class="number">1</span> , name, price[<span class="number">1</span>], game_evaluation[num], jump_link[num],Game_release_time[num],Game_os[num]]) </span><br><span class="line">            </span><br><span class="line">        num = num + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_data</span>(<span class="params">game_info</span>):</span> </span><br><span class="line">    save_path = <span class="string">&quot;Steam.csv&quot;</span> </span><br><span class="line">    df = pd.DataFrame(game_info, columns=[<span class="string">&#x27;排行榜&#x27;</span>, <span class="string">&#x27;游戏名字&#x27;</span>, <span class="string">&#x27;目前游戏价格₩&#x27;</span>, <span class="string">&#x27;游戏评价&#x27;</span>, <span class="string">&#x27;游戏页面链接&#x27;</span>,<span class="string">&#x27;游戏发售日期&#x27;</span>,<span class="string">&#x27;游戏支持系统&#x27;</span>]) </span><br><span class="line">    df.to_csv(save_path, index=<span class="number">0</span>) </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;文件保存成功！&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>: </span><br><span class="line">    Game_info = [] </span><br><span class="line">    <span class="comment"># 游戏全部信息 </span></span><br><span class="line">    Turn_link = [] </span><br><span class="line">    <span class="comment"># 翻页链接 </span></span><br><span class="line">    Jump_link = [] </span><br><span class="line">    <span class="comment"># 游戏详情页面链接 </span></span><br><span class="line">    Game_evaluation = [] </span><br><span class="line">    <span class="comment"># 游戏好评率和评价 </span></span><br><span class="line">    Game_release_time = []</span><br><span class="line">    <span class="comment"># 游戏发售日期</span></span><br><span class="line">    Game_os = []</span><br><span class="line">    <span class="comment"># 游戏支持系统</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>): </span><br><span class="line">        Turn_link.append(<span class="string">&quot;https://store.steampowered.com/search/?filter=globaltopsellers&amp;page=1&amp;os=win&quot;</span> + <span class="built_in">str</span>(<span class="string">&quot;&amp;page=&quot;</span> + <span class="built_in">str</span>(i))) </span><br><span class="line">        run(Game_info, Jump_link, Game_evaluation, get_text(Turn_link[i-<span class="number">1</span>]),Game_release_time,Game_os) </span><br><span class="line">    save_data(Game_info)</span><br></pre></td></tr></table></figure>
<p>目前的计划是每个周期爬游戏信息写到csv文件里，然后用flink去读取流数据并进行统计，做一个接口形式输出(或者输出到数据库里，再用springboot连接数据库去查询，可是有点麻烦)</p>
<p><font size=5><strong>Game</strong></font><br><br>&#160;&#160;&#160;&#160;1.「双人成行」永远滴神，惊喜继续，这周卡关比较少感觉体验又上了一个台阶，游戏中后期制作人的分镜水平发挥更好，打起来就是非常流畅。预计下周通关。<br><br>&#160;&#160;&#160;&#160;2.「最终幻想XV 皇家版」，好像是16年的时候被狂喷的烂作。ps5会免体验了一下意外的不错，jrpg味道很浓，画面表现好于ff7re。叙事有点不流程，但是战斗系统和bgm真的很棒，在路上开车听「solidarity」的时候真的蛮有感觉。<br></p>
<p><font size=5><strong>Others</strong></font><br><br>&#160;&#160;&#160;&#160;1.重读「海边的卡夫卡」。实在是太喜欢村上的「物语」味了，不管是奇妙的想象力还是各种隐喻，都特别吸引我，村上对暴力和system的「刺」也是我非常热衷的，如果说「奇鸟行状录」的刺是刺向战争，那么「海边的卡夫卡」是刺向父权，少年卡夫卡的弑父是必然的，他继承了父亲的诅咒，也打破了诅咒。（当然，我同意这里的父亲也是隐喻）<br><br>&#160;&#160;&#160;&#160;2.做题进度:<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;    #剑指 Offer 40 最小的k个数<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;#剑指 Offer 42 连续子数组的最大和<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;#剑指 Offer 47 礼物的最大价值<br></p>
]]></content>
      <categories>
        <category>周报</category>
      </categories>
      <tags>
        <tag>Logs</tag>
      </tags>
  </entry>
  <entry>
    <title>2021-04-19周报</title>
    <url>/2021/04/19/2021-04-19%E5%91%A8%E6%8A%A5/</url>
    <content><![CDATA[<p>&#160;&#160;&#160;&#160;这周需求也太多了，完全被排满了没有时间学东西，拿了几个午休的时间勉强把爬虫-&gt;flink-&gt;mysql-&gt;springboot+mybatis的路走通，计划爬虫-&gt;flink这一条路改成用kafka。周末把「双人成行」通了，ff15推进到第十二章，还看了「剃须。然后捡到女高中生」。去电影院看「指环王:护戒使者」跟jjm金姐焊接吃了顿烧肉（还蛮难吃）<br><br><font size=5><strong>Big Data</strong></font><br><br>&#160;&#160;&#160;&#160;1.阅读「<font color=black><a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/datastream_api.html">Flink DataStream API Programming Guide</a>」用于操作Flink数据流</font><br><br>&#160;&#160;&#160;&#160;2.阅读「<font color=black><a href="https://geek-docs.com/sql/sql-tutorials/how-are-sql-statements-executed.html">SQL 语句是如何执行的</a>」复习。<br><br>&#160;&#160;&#160;&#160;3.阅读「<font color=black><a href="https://mp.weixin.qq.com/s/x3ZPLxsYm-dcbecD8A27AQ">谷歌力推新语言 Logica，解决 SQL 重大缺陷！</a>」不看好。<br><br>&#160;&#160;&#160;&#160;4.实现springboot+mybatis查询每年发售游戏量的后端接口。</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-20.png width = "75%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">每年发售游戏数</font> 
</center>


<p><font size=5><strong>Game</strong></font><br><br>&#160;&#160;&#160;&#160;1.「双人成行」通关，很棒的游戏，9.5/10。全程非常享受，通关之后还去之前没有逛够的冰雪关卡玩了半天。<br><br>&#160;&#160;&#160;&#160;2.「最终幻想XV 皇家版」推进到12章。在第十章的时候遇到bug主线任务无法继续，只能读一个多小时前的档重来…不过游戏体验目前还是不错，战斗系统很耐玩，人物刻画得也不差，bgm也很屌。<br></p>
<p><font size=5><strong>Others</strong></font><br><br>&#160;&#160;&#160;&#160;1.「指环王:护戒使者」看得很爽，下周应该可以去看第二部。<br><br>&#160;&#160;&#160;&#160;2.做题进度:<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;本周忘记做题了。。。</p>
]]></content>
      <categories>
        <category>周报</category>
      </categories>
      <tags>
        <tag>Logs</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive谓词下推</title>
    <url>/2021/04/22/Hive%E8%B0%93%E8%AF%8D%E4%B8%8B%E6%8E%A8/</url>
    <content><![CDATA[<p>&#160;&#160;&#160;&#160;谓词下推并不是Hive特有的概念，应该说在所有关系型数据库的关系型查询Sql或者NoSql中都有谓词下推的概念。本文主要以Hive Sql为例，提供一些从谓词下推出发，HQL的优化思路。<br><br><br>&#160;&#160;&#160;&#160;<font size=5><strong>定义</strong></font><br><br>&#160;&#160;&#160;&#160;首先要理解什么是谓词下推。谓词下推的定义: <strong>将过滤表达式尽可能移动至靠近数据源的位置，以使真正执行时能直接跳过无关的数据。</strong>该定义来源网络，我个人认为有点不好理解。Talk is cheap,Let’s show the code.<br><br>&#160;&#160;&#160;&#160;比方说以下这段sql:<br></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tab1 <span class="keyword">join</span> tab2 <span class="keyword">on</span> tab1.id<span class="operator">=</span>tab2.id <span class="keyword">where</span> tab1.id<span class="operator">=</span><span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>&#160;&#160;&#160;&#160;从直觉上说，这一段Sql的执行计划应该是:<br><br>&#160;&#160;&#160;&#160;1.启动一个Mapper任务对tab1和tab2进行打标。<br><br>&#160;&#160;&#160;&#160;2.在Reducer上进行连接操作。<br><br>&#160;&#160;&#160;&#160;3.用在Reducer中用Filter Operator过滤出tab.id=1的数据作为输出。<br><br><br>&#160;&#160;&#160;&#160;但实际情况并非如此，这里用Odps maxcompute的Hive执行计划做参考。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">job0</span> <span class="string">is</span> <span class="string">root</span> <span class="string">job</span></span><br><span class="line"><span class="attr">In Job job0:</span></span><br><span class="line"><span class="attr">root Tasks:</span> <span class="string">M1</span></span><br><span class="line"><span class="attr">J2_1 depends on:</span> <span class="string">M1</span></span><br><span class="line"><span class="attr">In Task M1:</span></span><br><span class="line">    <span class="attr">Data source:</span> <span class="string">data_platform.tab1</span></span><br><span class="line">    <span class="attr">TS:</span> <span class="string">data_platform.tab2</span></span><br><span class="line">        <span class="attr">FIL:</span> <span class="string">IN(id,1L)</span></span><br><span class="line">            <span class="attr">RS: order:</span> <span class="string">+</span></span><br><span class="line">                <span class="attr">nullDirection:</span> <span class="string">*</span></span><br><span class="line">                <span class="attr">optimizeOrderBy:</span> <span class="literal">False</span></span><br><span class="line">                <span class="attr">valueDestLimit:</span> <span class="number">0</span></span><br><span class="line">                <span class="attr">dist:</span> <span class="string">HASH</span></span><br><span class="line">                <span class="attr">keys:</span></span><br><span class="line">                      <span class="string">id</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                      <span class="string">id</span> <span class="string">(bigint)</span></span><br><span class="line">                <span class="attr">partitions:</span></span><br><span class="line">                      <span class="string">id</span></span><br><span class="line"><span class="attr">In Task J2_1:</span></span><br><span class="line">    <span class="attr">SEL:</span> <span class="string">id</span></span><br><span class="line">            <span class="attr">SEL:</span> <span class="string">id</span></span><br><span class="line">                <span class="attr">JOIN:</span></span><br><span class="line">                     <span class="string">Project1</span> <span class="string">INNERJOIN</span> <span class="string">BufferOperator1</span></span><br><span class="line">                     <span class="attr">keys:</span></span><br><span class="line">                         <span class="number">0</span><span class="string">:id</span></span><br><span class="line">                         <span class="number">1</span><span class="string">:id</span></span><br><span class="line">                    <span class="attr">FS: output:</span> <span class="string">Screen</span></span><br><span class="line">                        <span class="attr">schema:</span></span><br><span class="line">                          <span class="string">id</span> <span class="string">(bigint)</span></span><br><span class="line">                          <span class="string">id</span> <span class="string">(bigint)</span> <span class="string">AS</span> <span class="string">id2</span></span><br><span class="line">            <span class="attr">JOIN:</span></span><br><span class="line">                 <span class="string">Project1</span> <span class="string">INNERJOIN</span> <span class="string">BufferOperator1</span></span><br><span class="line">                 <span class="attr">keys:</span></span><br><span class="line">                     <span class="number">0</span><span class="string">:id</span></span><br><span class="line">                     <span class="number">1</span><span class="string">:id</span></span><br><span class="line">                <span class="attr">FS: output:</span> <span class="string">Screen</span></span><br><span class="line">                    <span class="attr">schema:</span></span><br><span class="line">                      <span class="string">id</span> <span class="string">(bigint)</span></span><br><span class="line">                      <span class="string">id</span> <span class="string">(bigint)</span> <span class="string">AS</span> <span class="string">id2</span></span><br><span class="line"><span class="string">OK</span></span><br></pre></td></tr></table></figure>
<p>&#160;&#160;&#160;&#160;可以看到，实际情况并非如猜想的一般。odps中给出的执行计划可以这么解读<br><br>&#160;&#160;&#160;&#160;1.在Mapper进行Table Scan的时候启动了Filter Operator，过滤出满足条件的数据，同时进行打标。<br><br>&#160;&#160;&#160;&#160;2.在Reducer上进行连接操作。<br><br>&#160;&#160;&#160;&#160;这就是Hive中的谓词下推：<strong>通过提前执行连接语句中where或者on中的谓词，以达到在mapper端提前过滤数据，减少shuffle阶段网络传输量和磁盘IO量的目的，最终形成了Sql的优化。</strong><br><br>&#160;&#160;&#160;&#160;当然，以上只给出了一个例子。HQL中除了以上的Inner Join以外，还有Left Join,Right Join,Full Join等等，同时谓词也包括where谓词和on谓词。下面将通过官方文档查看Hive中是分别是怎么处理谓词下推的。<br><br><br>&#160;&#160;&#160;&#160;<font size=5><strong>官方文档解释</strong></font><br></p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-21.png width = "75%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">Hive官方文档</font> 
</center>



<table>
<thead>
<tr>
<th>定义</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td>主表</td>
<td>在outer join中需要返回所有数据的表叫做主表，比如left join中的左表</td>
</tr>
<tr>
<td>次表</td>
<td>在outer join中没有匹配到需要用null填充的表，比如left join中的右表</td>
</tr>
<tr>
<td>连接谓语</td>
<td>写在on后面 where之前的逻辑表达式 比如on t1.id=1</td>
</tr>
<tr>
<td>过滤谓语</td>
<td>写在where之后的逻辑表达式 比如where t1.id=1</td>
</tr>
</tbody></table>
<p>&#160;&#160;&#160;&#160;在大多数情况下，Hive中的谓词下推是自动的，该优化主要发生在编译过程中的生成逻辑执行计划(连接谓语优化)和优化逻辑执行计划(过滤谓语优化)中。但是Hive官方文档中也有点明，在以下两种情况下，Hive不会自动进行谓词下推:<br><br>&#160;&#160;&#160;&#160;1.对主表的连接谓词无法下推。<br><br>&#160;&#160;&#160;&#160;During Join predicates cannot be pushed past Preserved Row tables.<br><br>&#160;&#160;&#160;&#160;2.对次表的过滤谓词无法下推。<br>After Join predicates cannot be pushed past Null Supplying tables.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-22.png width = "75%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">Hive Sql编译过程</font> 
</center>
&#160;&#160;&#160;&#160;我们同样通过一个例子看看什么时候不会进行谓词下推：
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tab1 </span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> tab2 </span><br><span class="line"><span class="keyword">on</span> tab1.id<span class="operator">=</span>tab2.id</span><br><span class="line"><span class="keyword">and</span> tab1.id<span class="operator">!=</span><span class="number">1</span> <span class="keyword">and</span> tab2.id<span class="operator">!=</span><span class="number">1</span> </span><br><span class="line"><span class="keyword">where</span> tab1.id<span class="operator">!=</span><span class="number">2</span> <span class="keyword">and</span> tab2.id<span class="operator">!=</span><span class="number">2</span></span><br></pre></td></tr></table></figure>
我们看到tab1.id!=1这个谓词，符合主表/连接谓词这两个条件，应此不会进行下推。同时tab2.id!=2这个谓词，符合次表/过滤谓词这两个条件，所以也不会进行谓词下推。<br>
&#160;&#160;&#160;&#160;当然，我们也必须注意到以上两种情况是在不开启CBO情况下发生的，开启CBO的话以我在网路上的观察，可以对第二种情况进行优化，做到谓词下推。<br><br>
&#160;&#160;&#160;&#160;<font size=5>**优化思路**</font><br>
&#160;&#160;&#160;&#160;了解了Hive中谓词下推的处理方式以后，我们来稍微想一想有什么优化上的思路。我首先想到的是，和CBO一样对第二种情况进行优化，连接之前先通过一个子查询执行后面的where条件，想必能起到谓词下推的效果。除此之外，第一种情况是不是在部分情况也可以实现谓词下推，一个直观的印象就是:不要对主表进行除了连接词意外的连接谓词限制，尽量放到where条件中，这样才能实现谓词下推，达到优化的目的。]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>DataWareHouse</tag>
      </tags>
  </entry>
  <entry>
    <title>2021-04-25周报</title>
    <url>/2021/04/25/2021-04-25/</url>
    <content><![CDATA[<p>&#160;&#160;&#160;&#160;调休真的烦的批爆，习惯了双休突然又回来太难受了。周一提了离职，没想到leader和我同一天离职。然后就是需求需求需求，搞不懂为什么偏偏在快离职的时候有这么多需求，还是做完吧不要给其他人留坑。周五去团建太尬了也很无聊就应该直接不去。然后房子又是停电又是漏水的烦得很。突然有点想谈恋爱了上海好无聊555<br><br><font size=5><strong>Big Data</strong></font><br><br>&#160;&#160;&#160;&#160;1.阅读「<font color=black><a href="https://www.mdeditor.tw/dl/1p9aM">吴恩达：不要建立AI先行的企业，而要从任务导向开始 | 专访</a>」 “嘿，大家请以人工智能为先” ，团队就会把重点放在技术上，这也许对一个研究实验室来说挺好的。但做生意时，我一般是以客户或任务为导向，几乎没有以技术为导向过。</font><br><br>&#160;&#160;&#160;&#160;2.阅读「<font color=black><a href="https://blog.csdn.net/qq_23120963/article/details/104654185">HIVE学习三：partition和bucket及Join</a>」提炼出一种Join优化的新思路:Becket Join。<br><br>&#160;&#160;&#160;&#160;分桶类似于Reducer阶段做的Partition工作，用户自行指定桶数，在HDFS上分桶的体现是文件的形式(分区是文件夹)<br><br> &#160;&#160;&#160;&#160;当两个表的连接字段都进行了分桶，并且分桶数是互相的整数倍数的时候（说明分桶的方式是相似的），同时每个桶的大小都小于一个阈值，Hive会启动Bucket Join。在这个过程中，每个Mapper上只需要下载自己这个Bucket所需要的对应Bucket的数据到内存中就可以进行Hash Map Join,并不用启动reducer。但是要注意，Bucket Join的条件严苛，并且有额外的内存开销，并不总是能带来优化。(后续会更新一篇博客细讲)<br><br>&#160;&#160;&#160;&#160;3.阅读「<font color=black><a href="https://zhuanlan.zhihu.com/p/78266517">从一个sql引发的hive谓词下推的全面复盘及源码分析(上)</a>」超级好的文章。<br><br>&#160;&#160;&#160;&#160;4.成功用python发送kafka消息并用flink读取。</p>
<p><font size=5><strong>Game</strong></font><br><br>&#160;&#160;&#160;&#160;1.「最终幻想XV 皇家版」最后一章了，还有剧情补充dlc王者之剑，不过感觉dlc有点无聊，考虑跳过。<br></p>
<p><font size=5><strong>Others</strong></font><br><br>&#160;&#160;&#160;&#160;1.「指环王:双塔奇兵」草，第二部更爽了，什么时候上第三部啊急死了<br><br>&#160;&#160;&#160;&#160;2.  剧本「蜇蜂之死」量很大，玩起来蛮累的。<br><br>&#160;&#160;&#160;&#160;2.做题进度:<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;算法题没有新题，复习了几道经典题目。<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Hive:求每个用户的最大连续登陆天数<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Hive:求直播间最大同时在线人数<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Hive:求时间段内开播的直播间人数<br></p>
]]></content>
      <categories>
        <category>周报</category>
      </categories>
      <tags>
        <tag>Logs</tag>
      </tags>
  </entry>
  <entry>
    <title>数据仓库中事实表及部分维度表分类</title>
    <url>/2021/05/01/2021-05-01%E5%91%A8%E6%8A%A5/</url>
    <content><![CDATA[<p>&#160;&#160;&#160;&#160;数据仓库中总有些特定的事实表和维度表是平时不会去特地提起，但是在实践中离不开的，在《数据仓库工具箱中》,作者把事实表分成了三类，把维度表分成了无数类别(毕竟维度表是数仓的灵魂，与业务场景密切相关)。本文会简单提一下这三类事实表以及几类定义比较冷门的维度表。<br><br><br>&#160;&#160;&#160;&#160;<font size=5><strong>事实表</strong></font><br><br>&#160;&#160;&#160;&#160;1.事务事实表:每条记录一条记录，日期维度是事物发生的周期。<br><br>&#160;&#160;&#160;&#160;一个例子:一张业务系统后台提供的交易表，每一条记录都是一次交易，用trade_time记录交易时间。<br></p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-23.png width = "55%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">交易事实表</font> 
</center><br>
&#160;&#160;&#160;&#160;2.周期快照事实:每个时间段记录一次，日期维度是记录时间段的中止。<br>
&#160;&#160;&#160;&#160;一个例子:比分说有一张记录销售人员绩效的表，每隔1个小时会计算一次过去一个小时内某销售人员的销售额，则可以把记录时间统计时间作为改事实表的日期维度。<br>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-24.png width = "55%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">销售事实表</font> 
</center>

<p>&#160;&#160;&#160;&#160;3.累积快照事实表:记录不确定周期的数据，有多个日期维度，首次插入时需要用代理键处理，在未来更新。<br><br>&#160;&#160;&#160;&#160;一个例子:业务方除了提供一张交易表以外，同时还提供了订单完成表。可以把这两张表整合在一次以记录每一笔订单的整个周期<br></p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-25.png width = "55%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">订单事实表</font> 
</center>

<p>&#160;&#160;&#160;&#160;<font size=5><strong>维度表</strong></font><br></p>
<p>&#160;&#160;&#160;&#160;1.微型维度表:提取快速变化的维度，用度量值范围处理的维度表。<br><br>&#160;&#160;&#160;&#160;一个例子:一张用户维度表，其中的”收入”字段变化非常快，为了避免经常更新，用”收入范围”来代替这个维度。<br><br>&#160;&#160;&#160;&#160;2.支架表:把一些常用并且很大的维度做成支架表(比如说日期，地区，时间支架表)。支架表可以连接到维度表，也可以连接到事实表。这里部分支持了范式。也就是说在星型模型中部分使用了雪花模型。<br><br>&#160;&#160;&#160;&#160;一个例子:很多维度表和事实表都有用到日期，没有必要在所有维度表或者事实表中把日期相关的全部维度信息全部展开，不如在一个日期维度表中维度日期信息，这个日期维度表就是桥接表。<br></p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-27.png width = "55%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">日期支架表使用</font> 
</center>

<p>&#160;&#160;&#160;&#160;3.桥接表:当维度中出现多属性值时，可以通过桥接表存储多属性信息。<br><br>&#160;&#160;&#160;&#160;一个例子:一个直播事实表的主播信息可能存储一个主播的数组，可以把这个主播组合成一个id,再通过这个桥接表关联到具体的网红和网红信息<br></p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-28.png width = "85%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">主播组合桥接表</font> 
</center>

]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>DataWareHouse</tag>
      </tags>
  </entry>
  <entry>
    <title>2021-05-04周报</title>
    <url>/2021/05/04/2021-05-04%E5%91%A8%E6%8A%A5/</url>
    <content><![CDATA[<p>&#160;&#160;&#160;&#160;过了个五一假期，周报晚了…大概一天的样子。倒不如说这周也没什么内容可以放在周报的。回江门过了五一，很多时候我喜欢回江门的原因就是在家里什么都干不了，像时间停滞了一样，永远在吃饭睡觉玩手机循环。买了最新的macbook pro，积蓄也停住了。<br><br><font size=5><strong>Big Data</strong></font><br><br>&#160;&#160;&#160;&#160;1.阅读「<font color=black><a href="https://www.cnblogs.com/estellez/p/10147993.html">深入理解HDFS架构及原理</a>」 这部分主要看HDFS架构和读写原理。HDFS用的是master/slaver架构。又NameNode下达操作命令，DataNode处理存储和具体的读写操作。</font><br><br>&#160;&#160;&#160;&#160;2.阅读「<font color=black><a href="https://www.cnblogs.com/luengmingbiao/p/11333618.html">MapReduce On Yarn</a>」主要是了解一下Hadoop中Yarn是怎么调度MapReduce任务的。<br><br>  <center><br>    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-26.png width = "75%" alt=""/><br>    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">MapReduce On Yarn</font> </p>
</center>

<p>&#160;&#160;&#160;&#160;3.阅读「<font color=black><a href="https://www.cnblogs.com/bigdatalearnshare/p/13909000.html">Spark和MapReduce对比</a>」<br></p>
<p><font size=5><strong>Game</strong></font><br><br>&#160;&#160;&#160;&#160;1.「最终幻想XV 皇家版」好傻逼，王者之剑dlc无法切回本体了。我怀疑要坏档。<br></p>
<p><font size=5><strong>Others</strong></font><br><br>&#160;&#160;&#160;&#160;1.「无敌少侠」很爽的美漫。很看好。<br><br>&#160;&#160;&#160;&#160;2.「莫失莫忘」黑石一雄虽然算是日裔英国人，但这本书真的浓浓日本味…打算其他书也买来看一下。<br><br>&#160;&#160;&#160;&#160;3.做题进度:<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;做了好几道每日一题 and 牛客题，但是机场网很烂，就不登陆看了。</p>
]]></content>
      <categories>
        <category>周报</category>
      </categories>
      <tags>
        <tag>Logs</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里十一面血泪经</title>
    <url>/2021/07/21/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0JAVA%E5%BC%80%E5%8F%91%E4%B8%93%E5%AE%B6%E9%9D%A2%E7%BB%8F202105/</url>
    <content><![CDATA[<p>&#160;&#160;&#160;&#160;这个文章从我开始编辑到前两两天收到offer为止，已经编辑了两个月了，说是十一面，其实分成了两轮。第一轮面了六面由于工作经验不足最终被hr一票否决，然后换了个同部门的岗位面了五面后才成功拿到offer。<br><br><br>&#160;&#160;&#160;&#160;首先，先介绍一下本人的bg。211本科毕业，在上海某互联网小厂做了一年数仓，辞职准备读墨尔本大学硕士课程。拿到这个岗位的面试真的挺不可思议的。一开始是Boss直聘上有个阿里本地生活的高级开发问我对这个岗位有没有兴趣，由于我一直是做数仓的，JAVA开发基本上处于一知半解的状态，所以很老实地回答自己主要做数仓开发，但是他说没关系可以试试，所以就发了简历。<br><br><br>&#160;&#160;&#160;&#160;<font size=5><strong>简历面(时长15分钟)</strong></font><br><br>&#160;&#160;&#160;&#160;发了简历大概一个星期以后吧，某天早上突然收到了钉钉电话（由于当时刚离职一天，还在床上睡觉，接到电话整个人都是懵的）。本来以为只是简单了解一下情况，约面试时间什么的，没想到对方在简单自我介绍后就直接开始问题目。<br><br>&#160;&#160;&#160;&#160;<strong>题目一:用Hql实现以下查询</strong>:<br><br>&#160;&#160;&#160;&#160;成绩表:table，字端:id-&gt;自增键,uid-&gt;用户ID,date-&gt;交易日期,amount-&gt;交易金额<br><br>&#160;&#160;&#160;&#160;求连续三天交易额增长超过120%的用户    </p>
<table>
<thead>
<tr>
<th>id</th>
<th>uid</th>
<th>date</th>
<th>amount</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>1001</td>
<td>20210101</td>
<td>1.0</td>
</tr>
<tr>
<td>2</td>
<td>1002</td>
<td>20210102</td>
<td>12.0</td>
</tr>
<tr>
<td>3</td>
<td>1001</td>
<td>20210102</td>
<td>5.0</td>
</tr>
<tr>
<td>4</td>
<td>1004</td>
<td>20210103</td>
<td>3.0</td>
</tr>
<tr>
<td>&#160;&#160;&#160;&#160;<strong>题目二:Java关键字synchronized了解吗</strong>?<br></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>&#160;&#160;&#160;&#160;第一道题是口述Sql代码，由于当时确实没睡醒，上来有点懵，同时这段sql也确实比较复杂，一开始讲错了。不过讲了一会有意识到自己的错误，推倒重新来了一遍，面试官听完也比较认可。这道题就是连续登陆天数的变种，在处理连续之前先解决增长的问题就ok，有空把Hql的代码补充下。<br></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>&#160;&#160;&#160;&#160;第二道题确实是盲区，这个关键字我只知道和多线程有关，但是不知道详细的作用(毕竟工作中也没遇到过高并发场景)，所以直接回答不知道了。<br></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>&#160;&#160;&#160;&#160;第二个问题问完差不多就结束了，当时以为自己肯定凉了，不过回想起来挂电话前面试官有问我现在离职了是不是急着找工作，我破罐子破摔就说了实话，说没那么急，正在休假。后来才知道这是阿里的传统，突击电话。</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>&#160;&#160;&#160;&#160;<font size=5><strong>一面(时长60分钟)</strong></font><br><br>&#160;&#160;&#160;&#160;突击电话差不多过了四五天吧，又有个杭州的电话打过来，说他们是本地生活的，想约个面试。我一看邮件才发现这个岗位【数据平台JAVA开发专家】，想着我既不是JAVA开发也不是专家，怎么给我安排这个呢？还打电话回去确认了一下是不是弄错了，对方信号不好我也没听清说了什么，结论就是没弄错。最后决定反正不吃亏面面看吧。结果就是被血虐。<br><br>&#160;&#160;&#160;&#160;一面很直接，上来就先做题，发了个做题网站的邮件，里面4道题目。要求尽量在20mins内做完。不过题目都偏简单，也没有算法题。<br><br>&#160;&#160;&#160;&#160;<strong>题目一:JAVA实现单词倒序输出</strong>:<br><br>&#160;&#160;&#160;&#160;例子:输入一个”hello world java”字符串，输出”java world hello”字符串。<br><br>&#160;&#160;&#160;&#160;<strong>题目二:用Hql实现以下查询</strong>:<br><br>&#160;&#160;&#160;&#160;成绩表:table，字端:id-&gt;自增键,class-&gt;交班级,student-&gt;学生id,score-&gt;分数<br><br>&#160;&#160;&#160;&#160;求每个班级排名前10的学生信息。<br></p>
<table>
<thead>
<tr>
<th>id</th>
<th>student</th>
<th>class</th>
<th>score</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>1001</td>
<td>1</td>
<td>80</td>
</tr>
<tr>
<td>2</td>
<td>1002</td>
<td>1</td>
<td>90</td>
</tr>
<tr>
<td>3</td>
<td>1003</td>
<td>3</td>
<td>92</td>
</tr>
<tr>
<td>4</td>
<td>1004</td>
<td>2</td>
<td>78</td>
</tr>
<tr>
<td>&#160;&#160;&#160;&#160;<strong>题目三:用Hql实现以下查询</strong>:<br></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>&#160;&#160;&#160;&#160;成绩表:table，字端:id-&gt;自增键,name-&gt;名字<br></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>&#160;&#160;&#160;&#160;以id为准实现student的奇偶互换。(奇数行时，最后一行不换)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>&#160;&#160;&#160;&#160;例子:</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>|  id   |   name|<br>|  —-  | —-  |—|—|<br>| 1  | eric |<br>| 2  |zoe |<br>| 3  |jinny |<br>| 4  |timmon |<br>| 5  |sharon |<br>&#160;&#160;&#160;&#160;转化为</p>
<p>|  id   |   name|<br>|  —-  | —-  |—|—|<br>| 1  | zoe |<br>| 2  |eric |<br>| 3  |timmon |<br>| 4  |jinny |<br>| 5  |sharon |</p>
<p>&#160;&#160;&#160;&#160;<strong>题目四:用Hql实现以下查询</strong>:<br><br>&#160;&#160;&#160;&#160;成绩表:table，字端:id-&gt;自增键,number-&gt;数字<br><br>&#160;&#160;&#160;&#160;求连续出现3次以上的数字。    </p>
<table>
<thead>
<tr>
<th>id</th>
<th>number</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>5</td>
<td>2</td>
</tr>
<tr>
<td>&#160;&#160;&#160;&#160;题目比较简单，我差不多16分钟做完，花4分钟检查了一下。面试官大概看了几秒，说没什么问题就笔试就过了。然后就是对着简历问，这里问得很深，基本上简历里面提到的项目，包括工作上的学校里的，而且项目里面用过的技术，都问往原理那边问。我被问到的大概有以下问题：<br></td>
<td></td>
</tr>
<tr>
<td>&#160;&#160;&#160;&#160;1.dubbo接口原理(只说出来一点)<br></td>
<td></td>
</tr>
<tr>
<td>&#160;&#160;&#160;&#160;2.mapreduce原理，优化思路。（这个背得挺熟)<br></td>
<td></td>
</tr>
<tr>
<td>&#160;&#160;&#160;&#160;3.hdfs架构及原理。(只说出来一点）<br></td>
<td></td>
</tr>
<tr>
<td>&#160;&#160;&#160;&#160;4.clickhouse架构及原理。(答得还可以，不过当时没归纳所以有点乱)<br></td>
<td></td>
</tr>
<tr>
<td>&#160;&#160;&#160;&#160;5.熟悉哪些中间件，讲讲原理(讲了druid，原理说了一点点)。<br></td>
<td></td>
</tr>
<tr>
<td>&#160;&#160;&#160;&#160;6.聚集索引和非聚集索引。(记了个大概，答得不好)<br></td>
<td></td>
</tr>
<tr>
<td>&#160;&#160;&#160;&#160;7.java基础，架包依赖关系及一些常用的数据结构理解和原理(HashMap,HashTable,TreeMap),还有mybatis的一些细节问题。(答出来一半吧）<br></td>
<td></td>
</tr>
<tr>
<td>&#160;&#160;&#160;&#160;主要就这些，其中还穿插了一些多线程，springboot原理的问题，我直接说了不会。感觉真的是啥都问，快结束的时候我提了一嘴对实时数据感兴趣，最近在学flink。面试官立马接了一句：”那要不讲讲？“…（然后随便说了点很浅的东西）总之60分钟面得非常的满，基本上没有缓一缓的水问题，一面面完的时候实话说以为肯定凉了，因为很多问题都是能说出来一点但是深入一点就答不上来了，所以也没抱什么期望。</td>
<td></td>
</tr>
</tbody></table>
<p>&#160;&#160;&#160;&#160;<font size=5><strong>二面(时长45分钟)</strong></font><br><br>&#160;&#160;&#160;&#160;一面过后大概一个星期左右，又接了个电话，说要约后续面试，刚接到电话的时候我以为一面已经挂了，没想到还有后续，以为是一个新的面试流程，还问了是什么岗位，对方回答是同一岗位的二面，约的第二天晚上九点。<br></p>
<p>&#160;&#160;&#160;&#160;这一轮主要考察笔试，有算法题也有sql。<br><br>&#160;&#160;&#160;&#160;<strong>题目一:画圆</strong>:<br><br>&#160;&#160;&#160;&#160;输入：draw_point(x, y) x y 都是pos integer，能在屏幕的x y为坐标的地方画点<br><br>&#160;&#160;&#160;&#160;输出：draw_circle(x, y, r) x y r都是pos integer，能在屏幕的x y为圆心 r为半径的地方画圆<br><br>&#160;&#160;&#160;&#160;这题没有在网路上看过，估计是阿里的原创题。也不要求写出代码，给面试官一个思路即可。我当时说的思路就是在从一个点开始，在半径不变的情况下，x,y进行偏移。面试官说这个思路对，但是有没有优化的办法。当时确实想不出来，后来面试官直接把两个优化的点说出来了（现在记不太清，有兴趣的uu们可以想想)，感觉自己就做出来1/5吧。<br></p>
<p>&#160;&#160;&#160;&#160;<strong>题目二:段落切割</strong>:<br><br>&#160;&#160;&#160;&#160;输入：两个函数is_word(str)，一个字符串str<br><br>&#160;&#160;&#160;&#160;输出: 输出函数is_paragraph(str)判断输入的字符串能不能切割成一个段落<br><br>&#160;&#160;&#160;&#160;段落：能完美切割为一个个不含空格的单词。比如： hello是单词 world是单词。那么：helloworld是段落，hiworld就不是。<br><br>&#160;&#160;&#160;&#160;这道题完全翻车，那个时候做的题太少了，面试官提示回朔我也一点思路都没有。这个题leetcode有原型，leetcode139，最佳解法是动态规划。</p>
<p>&#160;&#160;&#160;&#160;<strong>题目三:SQL题</strong>:<br><br>&#160;&#160;&#160;&#160;表名：order_detail<br><br>&#160;&#160;&#160;&#160;字段名：order_id订单id，buyer_id买家id，amount金额,pay_time支付时间,item_id商品id<br><br>&#160;&#160;&#160;&#160;针对双十一期间的（20191111~20191113）交易，写出实现以下要求的SQL（要求1条SQL完成)找出：大额单（单笔订单金额超过1000.00元的订单） 数量超过3笔的买家<br>并输出他们的 买家id, 总订单数,  总订单金额, 是否总订单金额TOP 3<br><br>&#160;&#160;&#160;&#160;没啥好说的 hql+窗口函数嗯做就完事了</p>
<p>&#160;&#160;&#160;&#160;老实说这轮笔试也很糟糕，两道算法题几乎就没做出来，sql题倒是写得不错，然后基本上就是聊聊天，离职内容，优点缺点等等。这个时候我也以为自己凉了，结果最后反问一句你们现在在做什么，对方回答得很有耐心。大概给我讲了十五分钟吧，中间信号有问题还重拨过来继续讲。最后体验反而很好，这个反问环节也给我加了点信心。</p>
<p>&#160;&#160;&#160;&#160;<font size=5><strong>三面(时长45分钟)</strong></font><br><br>&#160;&#160;&#160;&#160;大概又过了一个星期,阿里终于又打电话过来了，大概8点钟左右问我今晚能不能面试？？我说我在地铁上来不及，才给我改到了第二天晚上。<br><br>&#160;&#160;&#160;&#160;这一轮基本上只有笔试了，题目出得很简单，同样是两道算法一道sql。<br></p>
<p>&#160;&#160;&#160;&#160;<strong>题目一:反转链表。</strong>:<br><br>&#160;&#160;&#160;&#160;leetcode206原题，也是面试热题了。很简单，双指针五分钟解决。<br></p>
<p>&#160;&#160;&#160;&#160;<strong>题目一:删除链表的倒数第 N 个结点</strong>:<br><br>&#160;&#160;&#160;&#160;leetcode19原题。也很简单，还是双指针十分钟解决。<br></p>
<p>&#160;&#160;&#160;&#160;<strong>题目三:SQL全局排序问题</strong>:<br><br>&#160;&#160;&#160;&#160;一个取top10的sql语句   select  * from xxx  order by  ccc   limit 10  这个是否做全排序了？怎么验证？能用sql实现让他不做全排序吗？这个问题怎么说呢，我当时下意识觉得不是全排序，但是题目明显引导我在往避免全排序去，所以写了个distribute by +sort by 。<br><br><br>&#160;&#160;&#160;&#160;但是事实上hive对topn是有优化的，从HIVE-3562开始，hive就将带有limit算子的order by下推至map端，这样map不必将所有数据shuffle到reduce端，而是在每个map端过滤出topn的数据后，再发送到reduce上做全局排序。具体原理如下：<br><br>&#160;&#160;&#160;&#160;ReduceSinkOperator内维护了一个TopNHash变量reducerHash,该变量决定了一条row是否被collect，也就是说是否被shuffle到下游。其中有一个非常重要的方法TopNHash.tryStoreKey()，把该行存到一个top N的heap中（最小堆)，这里通过TopNHash.tryStoreKey()的返回值进行尝试，如果符合条件则加入堆中：<br></p>
<p>&#160;&#160;&#160;&#160;面试题一次性写完，面试官看了20秒就说没问题，接着就问了几个Behavior question(最自豪的事情，优点缺点等）。这一轮面得很爽，笔试题做得超快，所以已经开始有点自信了。</p>
<p>&#160;&#160;&#160;&#160;<font size=5><strong>四面(时长45分钟)</strong></font><br><br>&#160;&#160;&#160;&#160;这一轮是技术总监面，没有笔试题，基本上都是一些比较抽象的问题，考察思考的深度。主要问题如下：<br><br>&#160;&#160;&#160;&#160;1.介绍项目<br><br>&#160;&#160;&#160;&#160;2.项目的价值怎么体现，设计上有什么支持扩展的思考<br><br>&#160;&#160;&#160;&#160;3.怎么从数据出发识别出物理世界中真实的人（一个人两台手机）<br><br>&#160;&#160;&#160;&#160;4.任务出现问题怎么处理 怎么实现告警机制全覆盖<br><br>&#160;&#160;&#160;&#160;5.618数据 如果出现问题来不及跑出来怎么办<br><br>&#160;&#160;&#160;&#160;6.平时抽象思考的框架是什么(…)<br><br>&#160;&#160;&#160;&#160;7.优点和缺点<br><br>&#160;&#160;&#160;&#160;8.反问环节<br><br>&#160;&#160;&#160;&#160;老实说这一轮面得不太好，很多问题没有预先想过。但反问环节听面试官的语气应该是过了。</p>
<p>&#160;&#160;&#160;&#160;<font size=5><strong>五面hr(时长45分钟)</strong></font><br><br>&#160;&#160;&#160;&#160;早先听说过阿里hr有一票否决权，但是万万没想到自己会挂在这里。。。<br><br>&#160;&#160;&#160;&#160;1.自我介绍<br><br>&#160;&#160;&#160;&#160;2.目标岗位工作内容<br><br>&#160;&#160;&#160;&#160;3.介绍项目<br><br>&#160;&#160;&#160;&#160;4.离职原因<br><br>&#160;&#160;&#160;&#160;5.最有成就感<br><br>&#160;&#160;&#160;&#160;6.反问环节<br><br>&#160;&#160;&#160;&#160;7.最大的坎<br><br>&#160;&#160;&#160;&#160;8.怎么push别人避免有压力<br><br>&#160;&#160;&#160;&#160;9.有没有其他公司<br><br>&#160;&#160;&#160;&#160;10.期望薪资<br><br>&#160;&#160;&#160;&#160;面完听hr语气感觉还是有点不太好，对方不太认同我的离职原因。<br></p>
<p>&#160;&#160;&#160;&#160;<font size=5><strong>六面技术加面(时长30分钟)</strong></font><br><br>&#160;&#160;&#160;&#160;正常流程是没有这一轮的，对方也跟我说了hr觉得工作经验不足，不给通过，所以技术的同学给我加一面，挖掘一下我其他地方的潜力（算法），希望能跟hr争取一下。问题如下<br><br>&#160;&#160;&#160;&#160;1.特征工程<br><br>&#160;&#160;&#160;&#160;2.特征提取<br><br>&#160;&#160;&#160;&#160;3.介绍一个机器学习方法（svm）<br><br>&#160;&#160;&#160;&#160;4.比赛<br><br>&#160;&#160;&#160;&#160;5.语音识别场景:饿了么的一个场景，怎么把商家的东西通过语音的形式输入生成菜单。（乱讲的一通，没想到对方觉得我的思路很好,还一直鼓励我说多点，然后我就顺着继续狂讲）<br><br>&#160;&#160;&#160;&#160;其实这一轮面试是最顺利的，对方也很满意（主要是场景题加分很多），结束的时候说觉得你很合适，但是hr那边有点难搞，他去努力看看。结果大概一周左右吧加了个微信告诉我没有搞定hr…</p>
<p>&#160;&#160;&#160;&#160;其实在这个时候，我已经放弃阿里了，甚至对阿里有点梁木，六次面试持续了一个多月搞得我心很累。没想到大概又过了一个星期，又有一个钉钉电话打过来，告诉我他们也是之前那个部门的，看到我的面试记录问我当时面试通过了怎么没有去？我说是因为工作年限的原因后对方让我要不再试试他们的【资深Java开发工程师-数据与算法工程方向】，我想了想试试也不吃亏就约了个面试。后来事实证明这一次面试体验好很多。</p>
<p>&#160;&#160;&#160;&#160;<font size=5><strong>一面(时长105分钟)</strong></font><br><br>&#160;&#160;&#160;&#160;出了两道算法题，此外对方对我的工作经历似乎不是特别感兴趣，反而在学校的算法经历上问了很多。<br></p>
<p>&#160;&#160;&#160;&#160;<strong>题目一:交错字符串</strong>:<br><br>&#160;&#160;&#160;&#160;leetcode97原题，中等难度。我在面试的一个星期前做过，官方题解是用动态规划，无奈当场完全没想起来，现场想了个回朔的方法。这道题做得还好，从我想起来回朔的方法就一直跟面试官沟通，蛮顺的把代码写出来了，但是因为边界条件多，要考虑的东西也多，感觉也写了有20分钟吧，好在对方比较耐心。<br></p>
<p>&#160;&#160;&#160;&#160;<strong>题目二:滑动窗口最大值</strong>:<br><br>&#160;&#160;&#160;&#160;leetcode239原题，困难难度。这是我在面试里遇到的第一个hard难度的算法题。先直接说了暴力的思路，面试官要求优化，然后就卡住了，在对方不断的提示下勉强有了一种思路，优先队列+按规则清除，和leetcode上的解法不一样。这个做法可能面试官没有想到，一下子没有找到反例（其实我当时想到了，可是不敢说），让我用java写出来就给我过了。这道题写了差不多40分钟，因为前面一直没有思路，毕竟hard难度的题平时都直接跳过的。最后在leetcode上卡了下官方题解才知道做法，当时确实写得不对…但是这道题跟面试官的沟通比较好，基本上一直在互相讲，最后写了份看起来没问题其实写错了的代码也蛮爽的。</p>
<p>&#160;&#160;&#160;&#160;做完题目又聊了40分钟，基本上也是对着简历问，主要问的方向是机器学习和clickhouse的东西，尤其是对我的美赛问了特别多。最后反问环节我很直接点出我上个面试是因为工作年限挂了，这次还会有这个问题吗。对方回答他们喜欢有潜力的，我的面评很好，而且对我的算法背景很感兴趣，还给我说了后面有哪些面试。总之这一轮面得挺好，聊的很不错，面完信心又回来了。</p>
<p>&#160;&#160;&#160;&#160;<font size=5><strong>二面(时长35分钟)</strong></font><br><br>&#160;&#160;&#160;&#160;可能我的面试记录比较多。后面的面试就再也没有笔试题了，这一轮问得还蛮水的，大段的时间都在机器学习离职原因优缺点上，具体的问题现在想不起来，总之没有什么值得提起的问题。面完当场说了会有下一面。<br></p>
<p>&#160;&#160;&#160;&#160;<font size=5><strong>三面(时长40分钟)</strong></font><br><br>&#160;&#160;&#160;&#160;这一轮是总监面。还是没有笔试，问题主要围绕项目，还有几个特征提取相关的场景题。场景题我答得没有特别好，我记得有一个问题是用户的流失这个指标要怎么定义怎么取数比较好。之前确实没有想过类似的问题，答起来磕磕绊绊的。这一轮其实很差劲，因为特征提取的问题之前没有思考过。最后也没想到居然能过。<br></p>
<p>&#160;&#160;&#160;&#160;<font size=5><strong>四面(时长12分钟)</strong></font><br><br>&#160;&#160;&#160;&#160;p10面，安排在周六。可能p10的老板真的很忙吧，我记得大概就是自我介绍，聊聊工作经历，问了下clickhouse的架构和原理加反问环节就结束了。clickhouse我准备了一大段内容，平时也得讲个5分钟左右，结果说了个开头就被打断了…最后跟我说我的工作经验有点少，他需要和前面的面试官对齐一下就结束了。我直接傻住，有种刚热完身比赛就结束了的感觉。<br></p>
<p>&#160;&#160;&#160;&#160;<font size=5><strong>五面(时长50分钟)</strong></font><br><br>&#160;&#160;&#160;&#160;因为上次倒在hr面，所以这次特地好好地准备了一下，包括但不限于一些润色过的故事，更新版本了的离职原因和聊天的风格。结果就是hr被我聊得很爽，中间甚至跟探讨了一下当下比较热门的剧本和北京这个城市。结束的时候态度也是“很希望你来”的感觉，甚至跟我说了一下其他互联网公司的坏话。总之这轮结束就我感觉到可以等offer了。<br></p>
<p>&#160;&#160;&#160;&#160;hr面过了大概三天吧，hr就电话联系说可以发offer还给我申请特批（第二天意向书就下来），最后定级了p5,薪资还算满意。阿里的这第二次面试体验还挺好的，从面试开始到拿到offer只花了两个星期，其中一面面试官也就是未来的leader还天天给我关注进度，基本上每天都要给我汇报情况和问我的情况，面试通过后又给我找房子又关注我体检进度的，有点感动。</p>
<p>&#160;&#160;&#160;&#160;大概就这样子！目前决定已经放弃墨尔本大学，去阿里上班了，希望这是一个好选择。</p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Interview</tag>
      </tags>
  </entry>
</search>
